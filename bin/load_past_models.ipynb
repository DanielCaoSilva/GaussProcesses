{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-14T04:44:55.446547514Z",
     "start_time": "2023-07-14T04:44:55.403329668Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import src.DataGrabber\n",
    "from src.utils import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale Max: 1677108352.0\n",
      " Scale Min: 1349069952.0\n",
      " Scale Factor: 328038400.0\n",
      " Before Block Reduce: (174818, 2)\n",
      "After Block Reduce: (7285, 2)\n",
      "Number of Nans: 0\n",
      "Start Time: 2012-09-30 16:55:44\n",
      "End Time: 2023-02-22 23:25:52\n",
      "Number of Days: 3642.0416666666665\n",
      "Time Period (Days): 3797.312592592593\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from src.utils import *\n",
    "from skimage.measure import block_reduce\n",
    "from datetime import datetime\n",
    "\n",
    "# plt.style.use('bmh')\n",
    "# plt.style.use('dark_background')\n",
    "# plt.style.use('Solarize_Light2')\n",
    "set_gpytorch_settings(False)\n",
    "\n",
    "# Reading data file and cleaning missing values\n",
    "df = pd.read_feather(\n",
    "    '../Data/feather/46221_9999_wave_height.feather')\n",
    "parameters_wave = ['time', 'wave_height']\n",
    "parameters_temp = ['time', 'sea_surface_temperature']\n",
    "df_as_np = df \\\n",
    "    .loc[:, parameters_wave] \\\n",
    "    .astype(float) \\\n",
    "    .replace(\n",
    "        to_replace=[999.0, 99.0, 9999.0],\n",
    "        value=np.nan) \\\n",
    "    .to_numpy()\n",
    "using_sk = block_reduce(\n",
    "    df_as_np, block_size=(24, 1),\n",
    "    func=np.mean).astype(float)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X = torch \\\n",
    "    .tensor(using_sk[:-1, 0]) \\\n",
    "    .float() \\\n",
    "    .cuda()\n",
    "y = torch \\\n",
    "    .tensor(using_sk[:-1, 1]) \\\n",
    "    .float() \\\n",
    "    .cuda()\n",
    "X = X.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "X = X[~torch.any(y.isnan(), dim=1)]\n",
    "y = y[~torch.any(y.isnan(), dim=1)]\n",
    "y = y.flatten()\n",
    "X_old = X\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def scaler(\n",
    "        a,\n",
    "        X_old=X_old,\n",
    "        center=True):\n",
    "    if center is True:\n",
    "        a = a - X_old.min(0).values\n",
    "    return a / (X_old.max(0).values - X_old.min(0).values)\n",
    "\n",
    "\n",
    "def add_new_kernel_term(\n",
    "        original_kernel, new_kernel_term, operation):\n",
    "    return str(original_kernel) + str(operation) + str(new_kernel_term)\n",
    "\n",
    "\n",
    "# GP Model Declaration\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(\n",
    "            self,\n",
    "            train_x_, train_y_,\n",
    "            likelihood, kernel):\n",
    "        super(ExactGPModel, self).__init__(train_x_, train_y_, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(\n",
    "            self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions \\\n",
    "            .MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "# Scale the time axis and log transform the Y-values\n",
    "X = scaler(X, X_old)\n",
    "y = y.log()\n",
    "\n",
    "# max, min, and scale factor declaration\n",
    "scaler_max = X_old.max(0).values.item()\n",
    "scaler_min = X_old.min(0).values.item()\n",
    "scale_factor = scaler_max - scaler_min\n",
    "scaler_consts = [scaler_max, scaler_min, scale_factor]\n",
    "\n",
    "# Plot the block reduced data set\n",
    "temp_for_plotting = pd.Series(\n",
    "    using_sk[:-1, 0] * 1e9, dtype='datetime64[ns]')\n",
    "# plt.plot(temp_for_plotting, using_sk[:-1, 1])\n",
    "# plt.xlabel(\"Time (epoch)\")\n",
    "# plt.ylabel(\"Significant Wave Height (meters)\")\n",
    "# plt.title(f'Significant wave height - after block reducing')\n",
    "# plt.show()\n",
    "\n",
    "print(\n",
    "    f'Scale Max: {scaler_max}\\n '\n",
    "    f'Scale Min: {scaler_min}\\n '\n",
    "    f'Scale Factor: {scale_factor}\\n '\n",
    "    f'Before Block Reduce: {df_as_np.shape}\\n'\n",
    "    f'After Block Reduce: {using_sk.shape}\\n'\n",
    "    f'Number of Nans: {np.count_nonzero(np.isnan(df_as_np))}\\n'\n",
    "    f'Start Time: {datetime.fromtimestamp(df_as_np[0, 0])}\\n'\n",
    "    f'End Time: {datetime.fromtimestamp(df_as_np[-1, 0])}\\n'\n",
    "    f'Number of Days: {df_as_np.shape[0] / 48}\\n'\n",
    "    f'Time Period (Days): {(df_as_np[-1, 0] - df_as_np[0, 0]) / 24 / 60 / 60}\\n ')\n",
    "\n",
    "# Prediction range, training and test set define (14, 3, 365)\n",
    "predict_days_out = 3\n",
    "test_n = 2 * predict_days_out\n",
    "\n",
    "# Split the data into train and test sets\n",
    "# *contiguous means they are sitting next to each other in memory*\n",
    "# train_x = X[test_n:].cuda()\n",
    "# train_y = y[test_n:].cuda()\n",
    "# test_x = X[-test_n:].cuda()\n",
    "# test_y = y[-test_n:].cuda()\n",
    "train_x = X[test_n:].contiguous().cuda()\n",
    "train_y = y[test_n:].contiguous().cuda()\n",
    "test_x = X[-test_n:].contiguous().cuda()\n",
    "test_y = y[-test_n:].contiguous().cuda()\n",
    "#\n",
    "# # Forecasting beyond horizon\n",
    "# test_future_15 = torch.cat((X[-test_n:], (X[1:(test_n*5)]+1)), dim=0).contiguous().cuda()\n",
    "# test_future_90 = torch.cat((X[-test_n:], (X[1:(test_n*30)]+1)), dim=0).contiguous().cuda()\n",
    "# train_x = X[test_n:].cuda()\n",
    "# train_y = y[test_n:].cuda()\n",
    "# test_x = X[-test_n:].cuda()\n",
    "# test_y = y[-test_n:].cuda()\n",
    "\n",
    "# Forecasting beyond horizon\n",
    "# test_future_15 = torch.cat((X[-test_n:], (X[1:(test_n*5)]+1)), dim=0).cuda()\n",
    "# test_future_90 = torch.cat((X[-test_n:], (X[1:(test_n*30)]+1)), dim=0).cuda()\n",
    "# print(test_future_15)\n",
    "# print(test_future_90)\n",
    "# print(test_x)\n",
    "\n",
    "# Create a list of random starting indices for the subtest sets\n",
    "n_total = train_x.shape[0]\n",
    "np.random.seed(2023)\n",
    "idx_list = np.random.randint(\n",
    "    low=n_total / 2,\n",
    "    high=n_total - test_n,\n",
    "    size=10)\n",
    "\n",
    "\n",
    "def make_idx_list(\n",
    "        training_set_size,\n",
    "        size_of_artificial_test_set,\n",
    "        size_of_partitions=1000, seed=2023):\n",
    "    np.random.seed(seed)\n",
    "    return np.random.randint(\n",
    "        low=training_set_size / 2,\n",
    "        high=training_set_size - size_of_artificial_test_set,\n",
    "        size=size_of_partitions)\n",
    "\n",
    "\n",
    "# Generate the train_loader and train_dataset\n",
    "train_loader, train_dataset, test_loader, test_dataset = create_train_loader_and_dataset(\n",
    "    train_x, train_y, test_x, test_y)\n",
    "data_compact = [\n",
    "    train_x, train_y, test_x, test_y,\n",
    "    train_loader, train_dataset,\n",
    "    test_loader, test_dataset]\n",
    "\n",
    "# List of possible Kernels operations\n",
    "kernel_operations = [\"+\", \"*\"]\n",
    "\n",
    "# List of possible Kernels terms\n",
    "kernel_list = [\n",
    "    # Periodic Kernels of Varying Period constraints\n",
    "    \"Per_Arb\", \"Per_Year\", \"Per_Season\", \"Per_Month\", \"Per_Week\",\n",
    "    # Random Fourier Features Kernel\n",
    "    \"RFF\",\n",
    "    # Varying Length Scales of the RBF Kernel\n",
    "    \"RQ\",\n",
    "    # Speciality Kernels\n",
    "    \"AR2\", \"Min\",\n",
    "    # Smoothing Kernels of the Matern class\n",
    "    \"RBF\", \"Mat_2.5\", \"Mat_1.5\", \"Mat_0.5\",\n",
    "]\n",
    "\n",
    "# Initial Kernel Trial\n",
    "kernel_str_running = \"AR2*RFF\"\n",
    "# kernel_str_running = \"RBF+AR2*Per_Year*RBF*Mat_1.5\"\n",
    "\n",
    "parameter_input = {\n",
    "    \"model_cls\": ExactGPModel,\n",
    "    \"kernel\": kernel_str_running,\n",
    "    \"train_x\": data_compact[0],\n",
    "    \"train_y\": data_compact[1],\n",
    "    \"test_x\": data_compact[2],\n",
    "    \"test_y\": data_compact[3],\n",
    "    \"scaler_min\": scaler_consts[1],\n",
    "    \"scaler_max\": scaler_consts[0],\n",
    "    \"num_iter\": 1000,\n",
    "    \"lr\": 0.01,\n",
    "    \"name\": kernel_str_running,\n",
    "    \"save_loss_values\": \"save\",\n",
    "    \"use_scheduler\": True,\n",
    "    \"forecast_over_this_horizon\": [4, 10], #None, #[test_future_15, test_future_90],\n",
    "    \"index_list_for_training_split\": idx_list,\n",
    "    \"predict_ahead_this_many_steps\": test_n,\n",
    "}\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T04:44:55.453605147Z",
     "start_time": "2023-07-14T04:44:55.446829699Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ExactGP:\n\tUnexpected key(s) in state_dict: \"mean_module.raw_constant\", \"covar_module.raw_outputscale\", \"covar_module.base_kernel.raw_lengthscale\", \"covar_module.base_kernel.raw_alpha\", \"covar_module.base_kernel.raw_lengthscale_constraint.lower_bound\", \"covar_module.base_kernel.raw_lengthscale_constraint.upper_bound\", \"covar_module.base_kernel.raw_alpha_constraint.lower_bound\", \"covar_module.base_kernel.raw_alpha_constraint.upper_bound\", \"covar_module.raw_outputscale_constraint.lower_bound\", \"covar_module.raw_outputscale_constraint.upper_bound\". ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m likelihood \u001B[38;5;241m=\u001B[39m gpytorch\u001B[38;5;241m.\u001B[39mlikelihoods\u001B[38;5;241m.\u001B[39mGaussianLikelihood()\n\u001B[1;32m      4\u001B[0m model \u001B[38;5;241m=\u001B[39m gpytorch\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39mExactGP(train_x, train_y, likelihood)  \u001B[38;5;66;03m#, kernel=state_dictionary.cov)\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_dictionary\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/MLconda/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict)\u001B[0m\n\u001B[1;32m   2036\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   2037\u001B[0m             \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2038\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(k) \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[1;32m   2040\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2041\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2042\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)))\n\u001B[1;32m   2043\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for ExactGP:\n\tUnexpected key(s) in state_dict: \"mean_module.raw_constant\", \"covar_module.raw_outputscale\", \"covar_module.base_kernel.raw_lengthscale\", \"covar_module.base_kernel.raw_alpha\", \"covar_module.base_kernel.raw_lengthscale_constraint.lower_bound\", \"covar_module.base_kernel.raw_lengthscale_constraint.upper_bound\", \"covar_module.base_kernel.raw_alpha_constraint.lower_bound\", \"covar_module.base_kernel.raw_alpha_constraint.upper_bound\", \"covar_module.raw_outputscale_constraint.lower_bound\", \"covar_module.raw_outputscale_constraint.upper_bound\". "
     ]
    }
   ],
   "source": [
    "list_of_past_models = glob.glob(\"./../Past_Trials/Model_States/*.pth\")\n",
    "state_dictionary = torch.load(list_of_past_models[0])\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = gpytorch.models.ExactGP(train_x, train_y, likelihood)  #, kernel=state_dictionary.cov)\n",
    "model.load_state_dict(state_dictionary)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T04:53:48.824012532Z",
     "start_time": "2023-07-14T04:53:48.797400792Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('likelihood.noise_covar.raw_noise', tensor([-0.1000])),\n             ('likelihood.noise_covar.raw_noise_constraint.lower_bound',\n              tensor(1.0000e-04)),\n             ('likelihood.noise_covar.raw_noise_constraint.upper_bound',\n              tensor(inf)),\n             ('mean_module.raw_constant', tensor(0.0960)),\n             ('covar_module.kernels.0.raw_outputscale', tensor(-0.0998)),\n             ('covar_module.kernels.0.base_kernel.raw_lengthscale',\n              tensor([[0.0995]])),\n             ('covar_module.kernels.0.base_kernel.raw_alpha',\n              tensor([0.0987])),\n             ('covar_module.kernels.0.base_kernel.raw_lengthscale_constraint.lower_bound',\n              tensor(0.0003)),\n             ('covar_module.kernels.0.base_kernel.raw_lengthscale_constraint.upper_bound',\n              tensor(inf)),\n             ('covar_module.kernels.0.base_kernel.raw_alpha_constraint.lower_bound',\n              tensor(0.0003)),\n             ('covar_module.kernels.0.base_kernel.raw_alpha_constraint.upper_bound',\n              tensor(inf)),\n             ('covar_module.kernels.0.raw_outputscale_constraint.lower_bound',\n              tensor(0.)),\n             ('covar_module.kernels.0.raw_outputscale_constraint.upper_bound',\n              tensor(inf)),\n             ('covar_module.kernels.1.raw_outputscale', tensor(-0.0999)),\n             ('covar_module.kernels.1.base_kernel.raw_lengthscale',\n              tensor([[0.0997]])),\n             ('covar_module.kernels.1.base_kernel.raw_period',\n              tensor([[-0.0962]])),\n             ('covar_module.kernels.1.base_kernel.raw_lengthscale_constraint.lower_bound',\n              tensor(0.0003)),\n             ('covar_module.kernels.1.base_kernel.raw_lengthscale_constraint.upper_bound',\n              tensor(inf)),\n             ('covar_module.kernels.1.base_kernel.raw_period_constraint.lower_bound',\n              tensor(1.0000e-04)),\n             ('covar_module.kernels.1.base_kernel.raw_period_constraint.upper_bound',\n              tensor(0.0050)),\n             ('covar_module.kernels.1.raw_outputscale_constraint.lower_bound',\n              tensor(0.)),\n             ('covar_module.kernels.1.raw_outputscale_constraint.upper_bound',\n              tensor(inf)),\n             ('covar_module.kernels.2.raw_outputscale', tensor(-0.0997)),\n             ('covar_module.kernels.2.base_kernel.raw_lengthscale',\n              tensor([[0.0980]])),\n             ('covar_module.kernels.2.base_kernel.raw_lengthscale_constraint.lower_bound',\n              tensor(0.0003)),\n             ('covar_module.kernels.2.base_kernel.raw_lengthscale_constraint.upper_bound',\n              tensor(inf)),\n             ('covar_module.kernels.2.raw_outputscale_constraint.lower_bound',\n              tensor(0.)),\n             ('covar_module.kernels.2.raw_outputscale_constraint.upper_bound',\n              tensor(inf))])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dictionary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T04:53:41.343664560Z",
     "start_time": "2023-07-14T04:53:41.331411263Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
