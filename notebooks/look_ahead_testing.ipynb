{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/dcaos/PycharmProjects/GaussProcesses/notebooks', '/home/dcaos/anaconda3/lib/python310.zip', '/home/dcaos/anaconda3/lib/python3.10', '/home/dcaos/anaconda3/lib/python3.10/lib-dynload', '', '/home/dcaos/.local/lib/python3.10/site-packages', '/home/dcaos/anaconda3/lib/python3.10/site-packages', '/home/dcaos/anaconda3/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-linux-x86_64.egg', '/home/dcaos/anaconda3/lib/python3.10/site-packages/GaussProcesses-0.1.0-py3.10.egg', '/home/dcaos/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg']\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(sys\u001b[39m.\u001b[39mpath)\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     18\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mmatplotlib\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minline\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mload_ext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mautoreload\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "sys.path.insert(0,Path(os.getcwd()).parent.__str__())\n",
    "# sys.path.append(Path(os.getcwd()).parent.__str__())\n",
    "import torch\n",
    "import gpytorch\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.measure import block_reduce\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import sys \n",
    "print(sys.path)\n",
    "from src.utils import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "plt.style.use('classic')\n",
    "set_gpytorch_settings(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Reading data file and cleaning missing values\n",
    "df = pd.read_feather('../Data/feather/46221_9999_wave_height.feather')\n",
    "parameters_wave = ['time', 'wave_height']\n",
    "parameters_temp = ['time', 'sea_surface_temperature']\n",
    "df_as_np = df \\\n",
    "    .loc[:, parameters_wave] \\\n",
    "    .astype(float) \\\n",
    "    .replace(to_replace=[999.0, 99.0, 9999.0], value=np.nan) \\\n",
    "    .to_numpy()\n",
    "using_sk = block_reduce(df_as_np, block_size=(24, 1), func=np.mean).astype(float)\n",
    "X = torch.tensor(using_sk[:-1, 0]).float().cuda()\n",
    "y = torch.tensor(using_sk[:-1, 1]).float().cuda()\n",
    "X = X.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "X = X[~torch.any(y.isnan(), dim=1)]\n",
    "y = y[~torch.any(y.isnan(), dim=1)]\n",
    "y = y.flatten()\n",
    "X_old = X\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def scaler(a, X_old=X_old, center=True):\n",
    "    if center is True:\n",
    "        a = a - X_old.min(0).values\n",
    "    return a / (X_old.max(0).values - X_old.min(0).values)\n",
    "\n",
    "\n",
    "def add_new_kernel_term(original_kernel, new_kernel_term, operation):\n",
    "    return str(original_kernel) + str(operation) + str(new_kernel_term)\n",
    "\n",
    "\n",
    "# GP Model Declaration\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, kernel):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "# Scale the time axis and log transform the Y-values\n",
    "X = scaler(X, X_old)\n",
    "y = y.log()\n",
    "\n",
    "# max, min, and scale factor declaration\n",
    "scaler_max = X_old.max(0).values.item()\n",
    "scaler_min = X_old.min(0).values.item()\n",
    "scale_factor = scaler_max - scaler_min\n",
    "scaler_consts = [scaler_max, scaler_min, scale_factor]\n",
    "print(f'Scale Max: {scaler_max}\\n Scale Min: {scaler_min}\\n Scale Factor: {scale_factor}')\n",
    "temp_for_plotting = pd.Series(using_sk[:-1, 0] * 1e9, dtype='datetime64[ns]')\n",
    "plt.plot(temp_for_plotting, using_sk[:-1, 1])\n",
    "plt.xlabel(\"Time (epoch)\")\n",
    "plt.ylabel(\"Significant Wave Height (meters)\")\n",
    "plt.title(f'Significant wave height - after block reducing')\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f'Before Block Reduce: {df_as_np.shape}\\n'\n",
    "    f'After Block Reduce: {using_sk.shape}\\n'\n",
    "    f'Number of Nans: {np.count_nonzero(np.isnan(df_as_np))}\\n'\n",
    "    f'Start Time: {datetime.fromtimestamp(df_as_np[0, 0])}\\n'\n",
    "    f'End Time: {datetime.fromtimestamp(df_as_np[-1, 0])}\\n'\n",
    "    f'Number of Days: {df_as_np.shape[0] / 48}\\n'\n",
    "    f'Time Period (Days): {(df_as_np[-1, 0] - df_as_np[0, 0]) / 24 / 60 / 60}')\n",
    "\n",
    "# Prediction range, training and test set define (14, 3, 365)\n",
    "predict_days_out = 28\n",
    "test_n = 2 * predict_days_out\n",
    "train_x = X[test_n:].contiguous().cuda()\n",
    "train_y = y[test_n:].contiguous().cuda()\n",
    "test_x = X[-test_n:].contiguous().cuda()\n",
    "test_y = y[-test_n:].contiguous().cuda()\n",
    "\n",
    "# Generate the train_loader and train_dataset\n",
    "train_loader, train_dataset, test_loader, test_dataset = create_train_loader_and_dataset(\n",
    "    train_x, train_y, test_x, test_y)\n",
    "data_compact = [train_x, train_y, test_x, test_y, train_loader, train_dataset, test_loader, test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = data_compact\n",
    "scale = scaler_consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exact_gp_obj = TrainTestPlotSaveExactGP(\n",
    "    ExactGPModel,\n",
    "    kernel=\"RBF+RFF*AR2+Min\",\n",
    "    train_x=data[0], train_y=data[1], test_x=data[2], test_y=data[3],\n",
    "    scaler_min=scale[1], scaler_max=scale[0],\n",
    "    num_iter=1000,\n",
    "    lr=0.01,\n",
    "    name=\"RBF+RFF*AR2+Min\",\n",
    "    save_loss_values=\"save\",\n",
    "    use_scheduler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bic_at_current, hyper_vals = exact_gp_obj.run_train_test_plot_kernel(set_xlim=[0.96, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_total = exact_gp_obj.train_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#idx_list = np.random.randint(n_total/2, n_total-6, size = 10000)\n",
    "idx_list = np.random.randint(n_total/2, n_total-6, size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "err_list = []\n",
    "\n",
    "for idx_ahead in idx_list:\n",
    "    print(iter)\n",
    "    with torch.no_grad():\n",
    "        # picks index in the second half of the observed data\n",
    "        print(idx_ahead)\n",
    "\n",
    "        # going to be the new \"training\" set; from t=0 to t=idx_ahead-1\n",
    "        temp_x_train = exact_gp_obj.train_x[:idx_ahead]\n",
    "        temp_y_train = exact_gp_obj.train_y[:idx_ahead]\n",
    "\n",
    "        # this is the new \"testing\" set; t=idx_ahead, ..., idx_ahead+6\n",
    "        temp_x_test = exact_gp_obj.train_x[idx_ahead:(idx_ahead+6)]\n",
    "        temp_y_test = exact_gp_obj.train_y[idx_ahead:(idx_ahead+6)]\n",
    "        exact_gp_obj.trained_model.set_train_data(inputs = temp_x_train, targets = temp_y_train, strict = False)\n",
    "\n",
    "        exact_gp_obj.trained_model.eval()\n",
    "        f = exact_gp_obj.trained_model(temp_x_test)\n",
    "\n",
    "        err = torch.mean((f.mean - temp_y_test).pow(2)).item()\n",
    "\n",
    "        err_list.append(err)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Recommendations:\n",
    "# 1. Add \"step_ahead_errors\" as a method to the exact_gp_obj class.  input: idx_list, output: err_list\n",
    "# 2. Generate idx_list, set seed first.  Then when you want the errors, you can feed it the same idx_list (fairness)\n",
    "# 3. Compare across models, average over the respective err_list and compare\n",
    "# Single out the top 2 models at each round, come up with these values.  Check for consistency with BIC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
