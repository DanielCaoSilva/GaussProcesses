{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm.notebook\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "import glob\n",
    "from skimage.measure import block_reduce\n",
    "import tqdm.notebook\n",
    "from datetime import datetime\n",
    "import gc\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "plt.style.use('classic')\n",
    "import src.utils\n",
    "from src.utils import set_gpytorch_settings\n",
    "set_gpytorch_settings(False)\n",
    "# Kernel Imports\n",
    "from gpytorch.kernels import PeriodicKernel, ProductStructureKernel, AdditiveStructureKernel, ScaleKernel, RBFKernel, MaternKernel, LinearKernel, PolynomialKernel, SpectralMixtureKernel, GridInterpolationKernel, InducingPointKernel, ProductKernel, AdditiveKernel, GridKernel\n",
    "from src.custom_kernel import MinKernel, AR2Kernel, MaternKernel, LinearKernel\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
    "# from gpytorch.kernels.keops import MaternKernel as FastMaternKernel\n",
    "# from gpytorch.kernels.keops import RBFKernel as FastRBFKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.constraints import Interval, GreaterThan\n",
    "import itertools\n",
    "# from gpytorch.metrics import mean_standardized_log_loss, quantile_coverage_error, mean_squared_error, mean_absolute_error\n",
    "from src.custom_kernel import noise_lower, noise_upper, noise_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reading data file and cleaning missing values\n",
    "df = pd.read_feather('../Data/feather/46221_9999_wave_height.feather')\n",
    "parameters_wave = ['time', 'wave_height']\n",
    "parameters_temp = ['time', 'sea_surface_temperature']\n",
    "df_as_np = df \\\n",
    "    .loc[:, parameters_wave] \\\n",
    "    .astype(float) \\\n",
    "    .replace(to_replace = [999.0, 99.0, 9999.0], value = np.nan) \\\n",
    "    .to_numpy()\n",
    "using_sk = block_reduce(df_as_np, block_size=(24,1), func=np.mean).astype(float)\n",
    "X = torch.tensor(using_sk[:-1,0]).float().cuda()\n",
    "y = torch.tensor(using_sk[:-1,1]).float().cuda()\n",
    "X = X.reshape(-1,1)\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "X = X[~torch.any(y.isnan(), dim=1)]\n",
    "y = y[~torch.any(y.isnan(), dim=1)]\n",
    "y = y.flatten()\n",
    "X_old = X\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def period_print(x, scale_factor_value):\n",
    "    print(f\"raw: {x}\")\n",
    "    print(f\"seconds: {x * scale_factor_value}\")\n",
    "    print(f\"minutes: {x * scale_factor_value / 60}\")\n",
    "    print(f\"hours: {x * scale_factor_value / 60 / 60 }\")\n",
    "    print(f\"days: {x * scale_factor_value / 60 / 60 / 24}\")\n",
    "    print(f\"weeks: {x * scale_factor_value / 60 / 60 / 24 / 7}\")\n",
    "    print(f\"months: {x * scale_factor_value / 60 / 60 / 24 / 30}\")\n",
    "    print(f\"years: {x * scale_factor_value / 60 / 60 / 24 / 365}\")\n",
    "\n",
    "def period_convert(x, type_to_convert, scale_factor_val):\n",
    "    match type_to_convert:\n",
    "        case \"raw\":\n",
    "            return x\n",
    "        case \"seconds\":\n",
    "            return x * scale_factor_val\n",
    "        case \"minutes\":\n",
    "            return x * scale_factor_val / 60\n",
    "        case \"hours\":\n",
    "            return x * scale_factor_val / 60 / 60\n",
    "        case \"days\":\n",
    "            return x * scale_factor_val / 60 / 60 / 24\n",
    "        case \"weeks\":\n",
    "            return x * scale_factor_val / 60 / 60 / 24 / 7\n",
    "        case \"months\":\n",
    "            return x * scale_factor_val / 60 / 60 / 24 / 30\n",
    "        case \"years\":\n",
    "            return x * scale_factor_val / 60 / 60 / 24 / 365\n",
    "\n",
    "def period_convert_list(li, type_to_convert, scale_factor_val):\n",
    "    converted_list = []\n",
    "    for h in li:\n",
    "        converted_list.append(period_convert(h, type_to_convert, scale_factor_val))\n",
    "    return converted_list\n",
    "\n",
    "def scaler(a, X_old=X_old, center=True):\n",
    "    if center is True:\n",
    "        a = a - X_old.min(0).values\n",
    "    return a / (X_old.max(0).values - X_old.min(0).values)\n",
    "\n",
    "def generate_kernel_instance(full_kernel_string):\n",
    "    kernel_class_parsed = full_kernel_string.split(\"_\")\n",
    "    kernel_class_type = kernel_class_parsed[0]\n",
    "    if len(kernel_class_parsed) > 1:\n",
    "        kernel_class_value = kernel_class_parsed[1]\n",
    "    else:\n",
    "        kernel_class_value = \"\"\n",
    "\n",
    "    match str(kernel_class_type):\n",
    "        case \"RBF\":\n",
    "            return copy.deepcopy(RBFKernel(\n",
    "                lengthscale_constraint=GreaterThan(\n",
    "                    0.000329)))\n",
    "        case \"Mat\":\n",
    "            nu_value = float(kernel_class_value[-3:])\n",
    "            return copy.deepcopy(MaternKernel(lengthscale_constraint=GreaterThan(\n",
    "                    0.000329), nu=nu_value))\n",
    "        case \"AR2\":\n",
    "            return copy.deepcopy(AR2Kernel(\n",
    "                period_constraint=Interval(\n",
    "                    lower_bound=1e-4, upper_bound=0.005),\n",
    "                lengthscale_constraint=GreaterThan(\n",
    "                    0.000329)))\n",
    "        case \"Per\":\n",
    "            match kernel_class_value:\n",
    "                case \"Arb\":\n",
    "                    return copy.deepcopy(PeriodicKernel(\n",
    "                    period_length_constraint=Interval(\n",
    "                        lower_bound=1e-4, upper_bound=0.005),\n",
    "                    lengthscale_constraint=GreaterThan(\n",
    "                        0.000329)))\n",
    "                case \"Week\":\n",
    "                    return copy.deepcopy(PeriodicKernel(\n",
    "                        period_length_constraint=Interval(\n",
    "                            lower_bound=1e-4, upper_bound=0.75,\n",
    "                            initial_value=scaler(60*60*24*7, center=False))))\n",
    "                case \"Month\":\n",
    "                    return copy.deepcopy(PeriodicKernel(\n",
    "                        period_length_constraint=Interval(\n",
    "                            lower_bound=1e-4, upper_bound=0.75,\n",
    "                            initial_value=scaler(60*60*24*30, center=False))))\n",
    "                case \"Year\":\n",
    "                    return copy.deepcopy(PeriodicKernel(\n",
    "                        period_length_constraint=Interval(\n",
    "                            lower_bound=1e-4, upper_bound=0.75,\n",
    "                            initial_value=scaler(60*60*24*365, center=False))))\n",
    "\n",
    "\n",
    "def make_kernel(name_of_kernel):\n",
    "    return_kernel_list = []\n",
    "    kernel_additive_terms = str(name_of_kernel).split('+')\n",
    "    for add_term_index, add_term in enumerate(kernel_additive_terms):\n",
    "        kernel_mult_terms = str(add_term).split(\"*\")\n",
    "        for mult_term_index, mult_term in enumerate(kernel_mult_terms):\n",
    "            return_kernel_list.append(mult_term)\n",
    "            if mult_term_index == 0:\n",
    "                cum_prod = generate_kernel_instance(mult_term)\n",
    "            else:\n",
    "                cum_prod = cum_prod * generate_kernel_instance(mult_term)\n",
    "        if add_term_index == 0:\n",
    "            cum_sum = copy.deepcopy(ScaleKernel(cum_prod))\n",
    "        else:\n",
    "            cum_sum = cum_sum + copy.deepcopy(ScaleKernel(cum_prod))\n",
    "    return copy.deepcopy(cum_sum)#, return_kernel_list\n",
    "\n",
    "def add_new_kernel_term(original_kernel, new_kernel_term, operation):\n",
    "    return str(original_kernel) + str(operation) + str(new_kernel_term)\n",
    "\n",
    "    # if name_of_kernel == 'RBF':\n",
    "    #     return RBFKernel(\n",
    "    #         lengthscale_constraint=GreaterThan(\n",
    "    #             0.00035\n",
    "    #         )\n",
    "    #     ).__getitem__(index)\n",
    "    # if name_of_kernel.startswith('Mat'):\n",
    "    #     nu_value = float(name_of_kernel[-3:])\n",
    "    #     return MaternKernel(lengthscale_constraint=GreaterThan(\n",
    "    #             0.00035\n",
    "    #         ), nu=nu_value)\n",
    "    # if name_of_kernel == 'AR2':\n",
    "    #     return AR2Kernel(\n",
    "    #         period_constraint=Interval(\n",
    "    #             lower_bound=1e-4, upper_bound=0.005),\n",
    "    #         lengthscale_constraint=GreaterThan(\n",
    "    #             0.00035\n",
    "    #         ))\n",
    "    # if name_of_kernel.startswith('Per'):\n",
    "    #     if name_of_kernel.startswith('Per_Arb'):\n",
    "    #         return PeriodicKernel(\n",
    "    #             period_length_constraint=Interval(\n",
    "    #                 lower_bound=1e-4, upper_bound=0.75)).__getitem__(index)\n",
    "    #     elif name_of_kernel == 'Per_Week':\n",
    "    #         return PeriodicKernel(\n",
    "    #             period_length_constraint=Interval(\n",
    "    #                 lower_bound=1e-4, upper_bound=0.75,\n",
    "    #             initial_value=scaler(60*60*24*7, center=False)))\n",
    "    #     elif name_of_kernel == 'Per_Month':\n",
    "    #         return PeriodicKernel(\n",
    "    #             period_length_constraint=Interval(\n",
    "    #                 lower_bound=1e-4, upper_bound=0.75,\n",
    "    #             initial_value=scaler(60*60*24*30, center=False)))\n",
    "    # else:\n",
    "    #     raise ValueError('Kernel not found')\n",
    "\n",
    "\n",
    "def descaler(a, X_old=X_old, center=True):\n",
    "    if center is True:\n",
    "        a = a * (X_old.max(0).values - X_old.min(0).values)\n",
    "    return a + X_old.min(0).values\n",
    "\n",
    "# GP Model Declaration\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, kernel):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = copy.deepcopy(kernel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make_kernel(\"RBF+Per_Arb*Per_Week\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from numpy.ma import indices\n",
    "\n",
    "# Scale the time axis and log transform the Y-values\n",
    "X = scaler(X, X_old)\n",
    "y = y.log()\n",
    "\n",
    "# max, min, and scale factor declaration\n",
    "scaler_max = X_old.max(0).values.item()\n",
    "scaler_min = X_old.min(0).values.item()\n",
    "scale_factor = scaler_max - scaler_min\n",
    "print(f'Scale Max: {scaler_max}\\n Scale Min: {scaler_min}\\n Scale Factor: {scale_factor}')\n",
    "temp_for_plotting = pd.Series(using_sk[:-1,0]*1e9, dtype='datetime64[ns]')\n",
    "plt.plot(temp_for_plotting, using_sk[:-1,1])\n",
    "\n",
    "# plt.plot(y.cpu().numpy())\n",
    "# print(y)\n",
    "plt.xlabel(\"Time (epoch)\")\n",
    "plt.ylabel(\"Significant Wave Height (meters)\")\n",
    "plt.title(f'Significant wave height - after block reducing')\n",
    "\n",
    "print(\n",
    "    f'Before Block Reduce: {df_as_np.shape}\\n'\n",
    "    f'After Block Reduce: {using_sk.shape}\\n'\n",
    "    f'Number of Nans: {np.count_nonzero(np.isnan(df_as_np))}\\n'\n",
    "    f'Start Time: {datetime.fromtimestamp(df_as_np[0,0])}\\n'\n",
    "    f'End Time: {datetime.fromtimestamp(df_as_np[-1,0])}\\n'\n",
    "    f'Number of Days: {df_as_np.shape[0]/48}\\n'\n",
    "    f'Time Period (Days): {(df_as_np[-1,0] - df_as_np[0,0]) / 24 / 60 / 60}')\n",
    "\n",
    "# Prediction range, training and test set define\n",
    "predict_days_out = 14\n",
    "test_n = 2*predict_days_out\n",
    "train_x = X[test_n:].contiguous().cuda()\n",
    "train_y = y[test_n:].contiguous().cuda()\n",
    "test_x = X[-test_n:].contiguous().cuda()\n",
    "test_y = y[-test_n:].contiguous().cuda()\n",
    "\n",
    "# Generate the train_loader and train_dataset\n",
    "train_loader, train_dataset, test_loader, test_dataset = src.utils.create_train_loader_and_dataset(\n",
    "    train_x, train_y, test_x, test_y)\n",
    "data_compact = [train_x, train_y, test_x, test_y, train_loader, train_dataset, test_loader, test_dataset]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List of possible Kernels terms\n",
    "# Long-term dependence\n",
    "smooth_kernel_list_t1 = ['RBF']#'', '', '', '']#, 'Mat0.5', 'Mat1.5', 'Mat2.5']\n",
    "# Medium-term dependence\n",
    "smooth_kernel_list_t2 = ['Mat2.5']#, 'Mat2.5']#['RBF', 'Mat0.5', 'Mat1.5',\n",
    "# Fixed Monthly Period\n",
    "periodic_kernel_list_t3 = ['Per_Month']\n",
    "# Cyclic period\n",
    "smooth_kernel_list_t4 = ['RBF']#, 'Mat0.5', 'Mat1.5', 'Mat2.5']\n",
    "periodic_kernel_list_t4 = ['Per_Arb']#['Per_Arb', 'Per_Week',\n",
    "\n",
    "periodic_kernel_list_tn = ['RBF', 'Per_Arb', 'Per_Arb', 'Per_Arb', 'Per_Arb', 'Per_Arb','', '', '', '', '', '', '']\n",
    "\n",
    "baseline_kernel_list = []\n",
    "for i in itertools.product(*[smooth_kernel_list_t1, smooth_kernel_list_t2, periodic_kernel_list_t3, periodic_kernel_list_t4, smooth_kernel_list_t4]):\n",
    "    baseline_kernel_list.append(i)\n",
    "\n",
    "n_kernel_list = []\n",
    "for i in itertools.combinations(periodic_kernel_list_tn, r=6):\n",
    "    n_kernel_list.append(i)\n",
    "n_k_l = list(pd.DataFrame(n_kernel_list).drop_duplicates(keep='first').iloc[:6, :].itertuples(index=False, name=None))\n",
    "n_k_l.reverse()\n",
    "\n",
    "\n",
    "kernel_operations = [\"+\", \"*\"]\n",
    "kernel_list = [\"Per_Arb\", \"RBF\", \"Per_Month\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(n_k_l)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initializing empty list to record values\n",
    "bic_save = []\n",
    "n = 0\n",
    "lr_check = 0.01#0.0063\n",
    "# for k_i in n_k_l:#baseline_kernel_list\n",
    "    # kernel = ScaleKernel(copy.deepcopy(make_kernel(k_i[0], index=n)))\n",
    "    # for t_i in list(k_i)[1:]:\n",
    "    #     if t_i == '':\n",
    "    #         break\n",
    "    #     kernel = kernel + (ScaleKernel(copy.deepcopy(make_kernel(t_i, index=n))))\n",
    "list_of_kernels_to_try = [\n",
    "    # ScaleKernel(\n",
    "    #     RBFKernel(lengthscale_constraint=GreaterThan(0.00035))),\n",
    "    # ScaleKernel(\n",
    "    #     MaternKernel(lengthscale_constraint=GreaterThan(0.00035), nu=2.5)),\n",
    "    # ScaleKernel(\n",
    "    #         PeriodicKernel(period_length_constraint=Interval(\n",
    "    #             lower_bound=1e-4, upper_bound=0.75))\n",
    "    #         *RBFKernel(lengthscale_constraint=GreaterThan(0.00035))),\n",
    "    # ScaleKernel(\n",
    "    #         PeriodicKernel(period_length_constraint=Interval(\n",
    "    #             lower_bound=1e-4, upper_bound=0.75,\n",
    "    #             initial_value=scaler(60*60*24*30, center=False)))\n",
    "    #         *RBFKernel(lengthscale_constraint=GreaterThan(0.00035))),\n",
    "    ScaleKernel(\n",
    "            PeriodicKernel(period_length_constraint=Interval(\n",
    "                lower_bound=1e-4, upper_bound=0.75,\n",
    "                initial_value=scaler(60*60*24*365, center=False)))\n",
    "            *RBFKernel(lengthscale_constraint=GreaterThan(0.00035)))\n",
    "]\n",
    "list_of_kernels_to_try_2 = [1]\n",
    "# for\n",
    "with_and_without_scheduler = [True]\n",
    "t_1, t_2, t_3, t_4, t_5 = 1, 0, 0, 0, 0\n",
    "# for iter_list in with_and_without_scheduler:\n",
    "#     if iter_list:\n",
    "#         lr_check = 0.01\n",
    "#     else:\n",
    "#         lr_check = 0.0063\n",
    "#     # kernel = copy.deepcopy(ScaleKernel(\n",
    "#     #     RBFKernel(lengthscale_constraint=GreaterThan(0.00035))))\n",
    "#     # kernel = copy.deepcopy(k_i) + kernel\n",
    "kernel_at_runtime = \\\n",
    "    ScaleKernel(\n",
    "        RBFKernel(lengthscale_constraint=GreaterThan(0.000329))) + \\\n",
    "    ScaleKernel(\n",
    "        PeriodicKernel(period_length_constraint=Interval(\n",
    "            lower_bound=1e-4, upper_bound=0.75)) * \\\n",
    "        RBFKernel(lengthscale_constraint=GreaterThan(0.000329))) #+ \\\n",
    "    # t_3 * ScaleKernel(\n",
    "    #     PeriodicKernel(period_length_constraint=Interval(\n",
    "    #         lower_bound=1e-4, upper_bound=0.75,\n",
    "    #         initial_value=scaler(60*60*24*365, center=False)) \\\n",
    "    #     * RBFKernel(lengthscale_constraint=GreaterThan(0.000329)))) + \\\n",
    "    # t_4 * ScaleKernel(\n",
    "    #         PeriodicKernel(period_length_constraint=Interval(\n",
    "    #             lower_bound=1e-4, upper_bound=0.75,\n",
    "    #             initial_value=scaler(60*60*24*30, center=False)) \\\n",
    "    #         * RBFKernel(lengthscale_constraint=GreaterThan(0.000329)))) + \\\n",
    "    # t_5 * ScaleKernel(\n",
    "    #     AR2Kernel(lengthscale_constraint=GreaterThan(0.000329))) \\\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # num_dims=6)\n",
    "    # sk1, sk2, pk1, pk2, sk3 = k_i\n",
    "    # kernel = ScaleKernel(make_kernel(sk1)) + ScaleKernel(make_kernel(sk2)) + ScaleKernel(make_kernel(pk1)) + ScaleKernel(make_kernel(pk2) * make_kernel(sk3))\n",
    "    # kernel_name = f'{sk1}plus{sk2}plus{pk1}plus{sk3}x{pk2}_exact_gp'\n",
    "    # k_list = [str(sk1), str(sk2), str(pk1), str(pk2), str(sk3)]\n",
    "bic_values = [10000]\n",
    "kernel_str_running = \"RBF\"\n",
    "while bic_values[-1] > 0:\n",
    "    for kernel_term_index, iter_kernel_terms in enumerate(kernel_list):\n",
    "        for ops_index, iter_ops in enumerate(kernel_operations):\n",
    "            kernel_str = add_new_kernel_term(kernel_str_running, iter_kernel_terms, iter_ops)\n",
    "            kernel = copy.deepcopy(make_kernel(kernel_str))\n",
    "            # kernel = kernel_at_runtime\n",
    "            exact_gp = src.utils.TrainTestPlotSaveExactGP(\n",
    "                ExactGPModel, kernel,\n",
    "                train_x, train_y, test_x, test_y,\n",
    "                num_iter=1000,\n",
    "                lr=lr_check, #lr=0.0063, #lr=0.01,\n",
    "                name=kernel_str,\n",
    "                save_loss_values=\"save\",\n",
    "                use_scheduler=True)\n",
    "            exact_gp.test_eval_exact_gp()\n",
    "            exact_gp.plot(show_plot=False)\n",
    "            bic_value = exact_gp.get_BIC()\n",
    "            bic_save.append([\n",
    "                kernel_str,\n",
    "                n,\n",
    "                bic_value.item(),\n",
    "                kernel,])\n",
    "            print(\"Kernel Structure (Best): \", kernel_str_running)\n",
    "            print(\"Kernel Structure (Current Trial): \", kernel_str)\n",
    "            if bic_value < bic_values[-1]:\n",
    "                bic_values.append(bic_value)\n",
    "                kernel_str_running = kernel_str\n",
    "            else:\n",
    "                kernel_str = kernel_str_running\n",
    "                pass\n",
    "            loss_df = pd.DataFrame(exact_gp.loss_values, columns=[\"Iterations\", \"Loss\"])\n",
    "            loss_df.plot(x=\"Iterations\", y=\"Loss\", kind='scatter')\n",
    "            loss_df.plot(x=\"Iterations\", y=\"Loss\", kind='hist')\n",
    "            loss_df.plot(x=\"Iterations\", y=\"Loss\", kind='kde')\n",
    "            loss_df.plot(x=\"Iterations\", y=\"Loss\", kind='box')\n",
    "\n",
    "            hyper_values = []\n",
    "            output_scale = []\n",
    "            # if n == 0:\n",
    "            #     output_scale.append([kernel.outputscale.item()])\n",
    "            # else:\n",
    "            #     output_scale.append([kernel.kernels[0].outputscale.item()])\n",
    "            # for pk in range(n+1):\n",
    "            #     print(pk)\n",
    "            #     if pk == 0:\n",
    "            #         hyper_values.append([-9999])\n",
    "            #     else:\n",
    "            #         hyper_values.append([kernel.kernels[pk].base_kernel.period_length.item()])\n",
    "            #         output_scale.append([kernel.kernels[pk].outputscale.item()])\n",
    "            # output_scale = [\n",
    "            #     kernel.kernels[0].outputscale.item(),\n",
    "            #     kernel.kernels[1].outputscale.item(),\n",
    "            #     kernel.kernels[2].outputscale.item(),\n",
    "            #     kernel.kernels[3].outputscale.item()\n",
    "            # ]\n",
    "            # hyper_values = [\n",
    "            #     kernel.kernels[0].base_kernel.lengthscale.item(),\n",
    "            #     kernel.kernels[1].base_kernel.lengthscale.item(),\n",
    "            #     kernel.kernels[2].base_kernel.lengthscale.item(),\n",
    "            #     kernel.kernels[2].base_kernel.period_length.item(),\n",
    "            #     kernel.kernels[3].base_kernel.kernels[0].lengthscale.item(),\n",
    "            #     kernel.kernels[3].base_kernel.kernels[0].period_length.item(),\n",
    "            #     kernel.kernels[3].base_kernel.kernels[1].lengthscale.item(),]\n",
    "\n",
    "                # loss_df])\n",
    "                #period_convert_list(hyper_values, \"days\", scale_factor),])\n",
    "                # *hyper_values,\n",
    "                # *k_i])\n",
    "\n",
    "            print(\"Learning Rate: \", lr_check)\n",
    "            # print(\"Kernel Structure: \", *k_i)\n",
    "            print(\"BIC: \", exact_gp.get_BIC().item())\n",
    "            print(\"Iterations Number(n): \",n)\n",
    "            # print(\"Hyper Values: \", hyper_values)\n",
    "            # print(\"Output Scale Values: \", output_scale)\n",
    "            # print(\"Memory Check: \", torch.cuda.mem_get_info(device=None))\n",
    "            # print(\"Kernel State Dict: \", kernel[0].state_dict())\n",
    "            for iter_k in kernel.named_parameters_and_constraints():#kernel.named_hyperparameters():\n",
    "                print(\"Kernel Params:\", iter_k[0], iter_k[2].transform(iter_k[1]).item())\n",
    "                # print(\"Kernel Params:\", iter_k[0], iter_k[-1].item())\n",
    "                # print(\"Kernel Params:\", iter_k[0], iter_k[-1].item())\n",
    "            # for iter_k in kernel.sub_kernels():\n",
    "            #     if n == 0:\n",
    "            #         continue\n",
    "            #     print(\"Kernel Iter outscale: \", iter_k.outputscale.item())\n",
    "            #     for iter_k_sub in iter_k.sub_kernels():\n",
    "            #         print(\"Kernel Iter Lengthscale: \", iter_k_sub.lengthscale.item())\n",
    "\n",
    "            # plt.plot(iterations_num, loss_vals)\n",
    "\n",
    "            n += 1\n",
    "            # lr_check += 0.0001\n",
    "            del exact_gp\n",
    "            del kernel\n",
    "            gc.enable()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "column_names = [\n",
    "        'Kernel_Name','n', 'BIC', 'Period_Length_(days)', 'Output_Scale', 'kernel_full', 'loss_df']\n",
    "        # 's1_ls_converted', 's2_ls_converted','pk1_ls_converted', 'pk1_pr_converted', 'pk2_ls_converted', 'pk2_pr_converted', 's3_ls_converted',\n",
    "        # 's1_ls_raw', 's2_ls_raw', 'pk1_ls_raw', 'pk1_pr_raw', 'pk2_ls_raw', 'pk2_pr_raw','s3_ls_raw',\n",
    "        # 'add_scale1_K1', 'add_scale2_K2', 'add_period1_K3', 'scale3_mult_K4', 'scale3_mult_K4']\n",
    "bic_out_df = pd.DataFrame(bic_save)\n",
    "    # bic_save, columns=column_names)\n",
    "\n",
    "# bic_out_df.to_csv('bic_save_linear_kernel_struct_baseline.csv')\n",
    "# bic_out_df.to_csv('bic_save_linear_kernel_struct_rbf_arb_additive.csv')\n",
    "bic_out_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# kernel\n",
    "# bic_out_df.to_csv(\"baseline_kernel_list_fixed_lower_bound_trial.csv\")\n",
    "#bic_out_df[\"kernel_full\"][3].kernelsP\n",
    "bic_out_df.to_csv(\"before_prez.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bic_out_df.iloc[:, 2:-1].to_latex()\n",
    "bic_out_df.iloc[:, 2:-1].to_latex(float_format=\"%.4f\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "period_print(0.000329, scale_factor)\n",
    "period_print(0.001419, scale_factor)\n",
    "period_print(2.62, scale_factor)\n",
    "# print(len(bic_save[0]))\n",
    "# bic_save\n",
    "# gc.enable()\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.CUDAGraph().pool()\n",
    "\n",
    "# baseline_df_master = pd.read_csv('Baseline_Kernel_Results.csv', index_col='BIC')\n",
    "# baseline_df_master.to_csv('Baseline_Kernel_Results_bak.csv')\n",
    "# # baseline_df_master = baseline_df_master.reset_index()\n",
    "# baseline_df_master\n",
    "\n",
    "\n",
    "\n",
    "# joined_baseline_df = pd.merge(right=baseline_df_master, left=bic_out_df, on='BIC', how='outer')\n",
    "# pd.concat([baseline_df_master, bic_out_df], ignore_index=True)\n",
    "# joined_baseline_df.to_csv('Baseline_Kernel_Results.csv')\n",
    "# joined_baseline_df\n",
    "# temp_list = []\n",
    "# temp_list.append([*period_convert_list(hyper_values, \"days\", scale_factor)])\n",
    "# temp_list\n",
    "\n",
    "\n",
    "# print(bic_out_df[\"kernel_full\"][3].kernels[0].outputscale.item(),\n",
    "#     bic_out_df[\"kernel_full\"][3].kernels[1].outputscale.item(),\n",
    "#     bic_out_df[\"kernel_full\"][3].kernels[2].outputscale.item(),\n",
    "#     bic_out_df[\"kernel_full\"][3].kernels[2].outputscale.item(),)\n",
    "# print(bic_out_df[\"kernel_full\"][3].kernels[0].outputscale.item()+bic_out_df[\"kernel_full\"][3].kernels[2].outputscale.item())\n",
    "\n",
    "\n",
    "\n",
    "# kernel.outputscale\n",
    "# kernel.kernels[2].base_kernel.lengthscale.item(),\n",
    "\n",
    "# for i in itertools.accumulate(periodic_kernel_list_tn, operator.add):\n",
    "#     print(i)\n",
    "# l1 = (['abc', 'def'])\n",
    "# l2 = ['123', '456']\n",
    "# l3 = ['---', 'xxx']\n",
    "# list_of_lists = [l1, l2, l3]\n",
    "# l4 = np.array([['rbf']])\n",
    "# # for i in itertools.product(zip(smooth_kernel_list_t1, smooth_kernel_list_t2, periodic_kernel_list_t3, smooth_kernel_list_t4, periodic_kernel_list_t4)):\n",
    "# for i in itertools.product(*[l1, l2, l3]):#*zip(l1, l2, l3)):\n",
    "# # for i in itertools.product(*[smooth_kernel_list_t1, smooth_kernel_list_t2, periodic_kernel_list_t3, smooth_kernel_list_t4, periodic_kernel_list_t4]):\n",
    "#     print(i, \": \")\n",
    "#     print(i[1])\n",
    "# # plt.plot(bic_out_df['BIC'])\n",
    "# # vector_i = np.array([[1, 1, 1, 0]]).T\n",
    "# # [a*b for a,b in zip(vector_i, l4)]\n",
    "# # np.dot(vector_i, l4)\n",
    "# kernel\n",
    "\n",
    "# two_term_results = pd.read_csv(\"joined_results_4_7.csv\")\n",
    "# merged_df = pd.merge(bic_out_df, two_term_results, how=\"outer\")\n",
    "# merged_df.to_csv('merged_csv_messy.csv')\n",
    "# merged_df\n",
    "\n",
    "# kernel\n",
    "# torch.cuda.current_device()\n",
    "print(.000329**2, .000390**2)\n",
    "# scale_factor = 328038400"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
