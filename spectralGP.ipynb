{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm.notebook\n",
    "import time\n",
    "import DataGrabber\n",
    "import utils\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "plt.style.use('classic')\n",
    "\n",
    "from utils import set_gpytorch_settings\n",
    "#gpytorch.settings.max_cholesky_size\n",
    "set_gpytorch_settings()\n",
    "\n",
    "# Command in terminal to help with memory allocation\n",
    "# set PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Kernel Imports\n",
    "from gpytorch.kernels import PeriodicKernel\n",
    "from custom_kernel import MinKernel, AR2Kernel, MaternKernel, LinearKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.constraints import Interval\n",
    "from custom_kernel import noise_lower, noise_upper, noise_init"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.6410e+09],\n        [1.6410e+09],\n        [1.6410e+09],\n        ...,\n        [1.6725e+09],\n        [1.6725e+09],\n        [1.6725e+09]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab Data and create tensors\n",
    "data_grab = DataGrabber.DataGrab() # year=[\"2020\", \"2021\", \"2022\"])\n",
    "wave_data = data_grab.grab_data()\n",
    "y = torch.tensor(wave_data['Wave Height'].values.astype(np.float32)).cuda()\n",
    "X = torch.tensor(wave_data['Time'].values.astype(np.float32)).cuda()\n",
    "X = X.reshape(-1,1)\n",
    "#wave_data\n",
    "X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Standardize Data\n",
    "X = (X - X.min(0).values) / (X.max(0).values - X.min(0).values)\n",
    "y = y.log()\n",
    "y = y - torch.min(y)\n",
    "y = 2 * (y / torch.max(y)) - 1\n",
    "# Training vs test\n",
    "from math import floor\n",
    "#train_n = int(floor(0.9 * len(X)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "test_n = 800\n",
    "train_x = X[test_n:].contiguous().cuda()\n",
    "train_y = y[test_n:].contiguous().cuda()\n",
    "test_x = X[-test_n:].contiguous().cuda()\n",
    "test_y = y[-test_n:].contiguous().cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000e+00],\n        [5.6827e-05],\n        [1.1365e-04],\n        ...,\n        [9.9989e-01],\n        [9.9994e-01],\n        [1.0000e+00]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(1.)\n",
      "tensor(0.4991)\n"
     ]
    }
   ],
   "source": [
    "print(X.min())\n",
    "print(X.max())\n",
    "print(X.mean())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0228)\n",
      "tensor(1.)\n",
      "tensor(0.5105)\n",
      "tensor(0.9772)\n",
      "tensor(1.)\n",
      "tensor(0.9886)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.min())\n",
    "print(train_x.max())\n",
    "print(train_x.mean())\n",
    "\n",
    "print(test_x.min())\n",
    "print(test_x.max())\n",
    "print(test_x.mean())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Generate the train_loader and train_dataset\n",
    "train_loader, train_dataset, test_loader, test_dataset = utils.create_train_loader_and_dataset(\n",
    "    train_x, train_y, test_x, test_y)\n",
    "data_compact = [train_x, train_y, test_x, test_y, train_loader, train_dataset, test_loader, test_dataset]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class StandardApproximateGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, kernel):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(-2))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "class MeanFieldApproximateGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, kernel):\n",
    "        variational_distribution = gpytorch.variational.MeanFieldVariationalDistribution(inducing_points.size(-2))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "class MAPApproximateGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, kernel):\n",
    "        variational_distribution = gpytorch.variational.DeltaVariationalDistribution(inducing_points.size(-2))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "def make_orthogonal_vs(model, train_x):\n",
    "    mean_inducing_points = torch.randn(1000, train_x.size(-1), dtype=train_x.dtype, device=train_x.device)\n",
    "    covar_inducing_points = torch.randn(100, train_x.size(-1), dtype=train_x.dtype, device=train_x.device)\n",
    "\n",
    "    covar_variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "        model, covar_inducing_points,\n",
    "        gpytorch.variational.CholeskyVariationalDistribution(covar_inducing_points.size(-2)),\n",
    "        learn_inducing_locations=True\n",
    "    )\n",
    "\n",
    "    variational_strategy = gpytorch.variational.OrthogonallyDecoupledVariationalStrategy(\n",
    "        covar_variational_strategy, mean_inducing_points,\n",
    "        gpytorch.variational.DeltaVariationalDistribution(mean_inducing_points.size(-2)),\n",
    "    )\n",
    "    return variational_strategy\n",
    "\n",
    "class OrthDecoupledApproximateGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, kernel):\n",
    "        variational_distribution = gpytorch.variational.DeltaVariationalDistribution(inducing_points.size(-2))\n",
    "        variational_strategy = make_orthogonal_vs(self, train_x)\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class SpectralDeltaGP(gpytorch.models.ExactGP):\n",
    "    # def __init__(self, train_x, train_y, kernel, num_deltas, noise_init=None):\n",
    "    #     likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint=gpytorch.constraints.GreaterThan(1e-11))\n",
    "    #     likelihood.register_prior(\"noise_prior\", gpytorch.priors.HorseshoePrior(0.1), \"noise\")\n",
    "    #     likelihood.noise = 1e-2\n",
    "    def __init__(self, inducing_points, kernel):\n",
    "        variational_distribution = gpytorch.variational.DeltaVariationalDistribution(inducing_points)\n",
    "        #variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(-2))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        #super(SpectralDeltaGP, self).__init__(train_x, train_y, likelihood)\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        #base_covar_module = kernel #gpytorch.kernels.SpectralDeltaKernel(num_dims=train_x.size(-1), num_deltas=num_deltas)\n",
    "        #base_covar_module.initialize_from_data(train_x[0], train_y[0])\n",
    "        self.covar_module = kernel#gpytorch.kernels.ScaleKernel(base_covar_module)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "\n",
    "#likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "#model = SpectralMixtureGPModel(train_x, train_y, likelihood)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from gpytorch.kernels import SpectralMixtureKernel\n",
    "\n",
    "num_deltas = 300\n",
    "kernel_old = (\n",
    "        #ScaleKernel(AR2Kernel()) +\n",
    "        #ScaleKernel(MinKernel()*RBFKernel()) +\n",
    "        #ScaleKernel(MinKernel())+\n",
    "        #ScaleKernel(RBFKernel()) +\n",
    "        #ScaleKernel(RBFKernel()*LinearKernel())+\n",
    "        ScaleKernel(MaternKernel(nu=0.5)) +\n",
    "        ScaleKernel(MaternKernel(nu=1.5)*PeriodicKernel(period_length_constraint=Interval(\n",
    "            lower_bound=1e-4,\n",
    "            upper_bound=0.1,\n",
    "            initial_value=0.01\n",
    "        ))) +\n",
    "        ScaleKernel(MaternKernel(nu=1.5)*PeriodicKernel(period_length_constraint=Interval(\n",
    "            lower_bound=0.05,\n",
    "            upper_bound=0.3,\n",
    "            initial_value=0.15\n",
    "        ))) +\n",
    "        ScaleKernel(MaternKernel(nu=1.5)*PeriodicKernel(period_length_constraint=Interval(\n",
    "            lower_bound=0.25,\n",
    "            upper_bound=1,\n",
    "            initial_value=0.5\n",
    "        ))) +\n",
    "        ScaleKernel(gpytorch.kernels.SpectralDeltaKernel(\n",
    "            num_dims=train_x.size(-1),\n",
    "            num_deltas=num_deltas,\n",
    "        )) +\n",
    "        ScaleKernel(gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4))\n",
    "        #ScaleKernel(RBFKernel()*PeriodicKernel())\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dcaos\\anaconda3\\envs\\GPs\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "k4_spectral_base = SpectralMixtureKernel(num_mixtures=4)\n",
    "k4_spectral_base.initialize_from_data_empspect(train_x, train_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "k1 = copy.deepcopy(k4_spectral_base)\n",
    "\n",
    "k2 = copy.deepcopy(k4_spectral_base) *PeriodicKernel(\n",
    "    period_length_constraint=Interval(\n",
    "    lower_bound=0.25,\n",
    "    upper_bound=1,\n",
    "    initial_value=0.5))\n",
    "k3 = ScaleKernel(LinearKernel())\n",
    "\n",
    "k4 = ScaleKernel(MaternKernel(nu=1.5)*PeriodicKernel(\n",
    "    period_length_constraint=Interval(\n",
    "    lower_bound=1e-4,\n",
    "    upper_bound=0.25,\n",
    "    initial_value=0.01)))\n",
    "kernel = (\n",
    "\tk1 + k3 + k4\n",
    "    # not: ScaleKernel(k1+k2) since they all have spectral component\n",
    ")\n",
    "\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = Interval(noise_lower, noise_upper,initial_value=noise_init))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Training StandardApproximateGP:   0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac60ba527f744c55b6c4780eaa3455d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dcaos\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\utils\\cholesky.py:38: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "C:\\Users\\dcaos\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\utils\\cholesky.py:38: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_ind_pts = 800 # Number of inducing points (128 is default for train_and_test_approximate_gp function)\n",
    "num_epochs = 10\n",
    "\n",
    "#m1, l1 = utils.train_and_test_approximate_gp(\n",
    "#    StandardApproximateGP, kernel, *data_compact, num_epochs=100, num_ind_pts=num_ind_pts)\n",
    "# m2, l2 = utils.train_and_test_approximate_gp(\n",
    "#     MeanFieldApproximateGP, kernel, *data_compact, num_epochs=100, num_ind_pts=num_ind_pts)\n",
    "# m3, l3 = utils.train_and_test_approximate_gp(\n",
    "#     MAPApproximateGP, kernel, *data_compact, num_epochs=100, num_ind_pts=num_ind_pts)\n",
    "# m4, l4 = utils.train_and_test_approximate_gp(\n",
    "#     OrthDecoupledApproximateGP, kernel, *data_compact, num_epochs=100, num_ind_pts=num_ind_pts)\n",
    "# m5, l5 = utils.train_and_test_approximate_gp(\n",
    "#     SpectralDeltaGP, kernel, *data_compact, num_epochs=100, num_ind_pts=num_deltas)\n",
    "# m6, l6 = utils.train_and_test_approximate_gp(\n",
    "#     SpectralMixtureGPModel, kernel, *data_compact, num_epochs=100, num_ind_pts=num_deltas)\n",
    "#l1 = gpytorch.likelihoods.GaussianLikelihood()\n",
    "#m1 = SpectralMixtureGPModel(train_x, train_y, likelihood)\n",
    "m1, l1 = utils.train_and_test_approximate_gp(\n",
    "    StandardApproximateGP, kernel, *data_compact, num_epochs=num_epochs, num_ind_pts=num_ind_pts)\n",
    "m2, l2 = utils.train_and_test_approximate_gp(\n",
    "    OrthDecoupledApproximateGP, kernel, *data_compact, num_epochs=num_epochs, num_ind_pts=num_ind_pts)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pairs = [[m1, l1],\n",
    "        [m2, l2],]# [m3, l3],\n",
    "#         [m4, l4], [m5, l5],\n",
    "        #[m1, l1]]\n",
    "\n",
    "for pair in pairs:\n",
    "    model = pair[0]\n",
    "    likelihood = pair[1]\n",
    "    model.eval()\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x[:,0].detach().cpu().numpy(),\n",
    "                    lower.detach().cpu().numpy(),\n",
    "                    upper.detach().cpu().numpy(), alpha=0.3)\n",
    "    # Plot training data as black stars\n",
    "    ax.scatter(train_x[:,0].detach().cpu().numpy(), train_y.detach().cpu().numpy(), s=0.5)\n",
    "    #ax.scatter(model.variational_strategy.inducing_points[:,0].detach().cpu().numpy(),\n",
    "    #           np.zeros(500)+1, s=0.5)\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x[:,0].detach().cpu().numpy(), observed_pred.mean.detach().cpu().numpy(), 'blue')\n",
    "    ax.scatter(\n",
    "        test_x[:,0].detach().cpu().numpy(),\n",
    "        test_y.detach().cpu().numpy(),\n",
    "        s=1, color=\"red\")\n",
    "    ax.set_xlim(.8,1)\n",
    "    #ax.set_xlim(0.65,0.75)\n",
    "    ax.vlines(m1.variational_strategy.inducing_points.detach().cpu().numpy(), ymin = -1.6, ymax = -1.5)\n",
    "\n",
    "    #ax.set_ylim([0, 1.5])\n",
    "    #ax.patch.set_facecolor('green')\n",
    "    #ax.patch.set_alpha(.1)\n",
    "    ax.legend([\"95% Credible Intervals\", \"Observed Data\", \"Posterior Mean\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([400, 1])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0228],\n        [0.0229],\n        [0.0229],\n        ...,\n        [0.9999],\n        [0.9999],\n        [1.0000]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    m1(test_x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |    9605 KB |    1047 MB |     775 GB |     775 GB |\\n|       from large pool |    7656 KB |    1029 MB |     754 GB |     754 GB |\\n|       from small pool |    1948 KB |      20 MB |      20 GB |      20 GB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |    9605 KB |    1047 MB |     775 GB |     775 GB |\\n|       from large pool |    7656 KB |    1029 MB |     754 GB |     754 GB |\\n|       from small pool |    1948 KB |      20 MB |      20 GB |      20 GB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |    1240 MB |    1240 MB |    1240 MB |       0 B  |\\n|       from large pool |    1216 MB |    1216 MB |    1216 MB |       0 B  |\\n|       from small pool |      24 MB |      24 MB |      24 MB |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |   19067 KB |  329960 KB |  182371 MB |  182353 MB |\\n|       from large pool |    8727 KB |  321792 KB |  160013 MB |  160005 MB |\\n|       from small pool |   10339 KB |   17031 KB |   22357 MB |   22347 MB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     173    |     415    |  381990    |  381817    |\\n|       from large pool |       1    |      61    |   47240    |   47239    |\\n|       from small pool |     172    |     355    |  334750    |  334578    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     173    |     415    |  381990    |  381817    |\\n|       from large pool |       1    |      61    |   47240    |   47239    |\\n|       from small pool |     172    |     355    |  334750    |  334578    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      47    |      47    |      47    |       0    |\\n|       from large pool |      35    |      35    |      35    |       0    |\\n|       from small pool |      12    |      12    |      12    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      25    |      58    |  221954    |  221929    |\\n|       from large pool |       1    |      26    |   25025    |   25024    |\\n|       from small pool |      24    |      48    |  196929    |  196905    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(len(m1.covar_module.kernels)):\n",
    "#     print(m1.covar_module.kernels[i].outputscale)\n",
    "torch.cuda.memory_summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 4.33 GiB (GPU 0; 8.00 GiB total capacity; 4.78 GiB already allocated; 949.99 MiB free; 5.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [20]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_BIC\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mget_BIC\u001B[49m\u001B[43m(\u001B[49m\u001B[43mm1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlikelihood\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_y\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_x\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\GaussProcesses\\utils.py:12\u001B[0m, in \u001B[0;36mget_BIC\u001B[1;34m(model, likelihood, y, X_std)\u001B[0m\n\u001B[0;32m     10\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     11\u001B[0m mll \u001B[38;5;241m=\u001B[39m gpytorch\u001B[38;5;241m.\u001B[39mmlls\u001B[38;5;241m.\u001B[39mExactMarginalLogLikelihood(likelihood, model)\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m---> 12\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_std\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m l \u001B[38;5;241m=\u001B[39m mll(f, y)  \u001B[38;5;66;03m# log marginal likelihood\u001B[39;00m\n\u001B[0;32m     14\u001B[0m num_param \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(p\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mhyperparameters())\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\models\\approximate_gp.py:81\u001B[0m, in \u001B[0;36mApproximateGP.__call__\u001B[1;34m(self, inputs, prior, **kwargs)\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m     80\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 81\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvariational_strategy(inputs, prior\u001B[38;5;241m=\u001B[39mprior, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\variational\\variational_strategy.py:169\u001B[0m, in \u001B[0;36mVariationalStrategy.__call__\u001B[1;34m(self, x, prior, **kwargs)\u001B[0m\n\u001B[0;32m    166\u001B[0m         \u001B[38;5;66;03m# Mark that we have updated the variational strategy\u001B[39;00m\n\u001B[0;32m    167\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdated_strategy\u001B[38;5;241m.\u001B[39mfill_(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m--> 169\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(x, prior\u001B[38;5;241m=\u001B[39mprior, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\variational\\_variational_strategy.py:124\u001B[0m, in \u001B[0;36m_VariationalStrategy.__call__\u001B[1;34m(self, x, prior, **kwargs)\u001B[0m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;66;03m# Get q(f)\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(variational_dist_u, MultivariateNormal):\n\u001B[1;32m--> 124\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(\n\u001B[0;32m    125\u001B[0m         x,\n\u001B[0;32m    126\u001B[0m         inducing_points,\n\u001B[0;32m    127\u001B[0m         inducing_values\u001B[38;5;241m=\u001B[39mvariational_dist_u\u001B[38;5;241m.\u001B[39mmean,\n\u001B[0;32m    128\u001B[0m         variational_inducing_covar\u001B[38;5;241m=\u001B[39mvariational_dist_u\u001B[38;5;241m.\u001B[39mlazy_covariance_matrix,\n\u001B[0;32m    129\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    130\u001B[0m     )\n\u001B[0;32m    131\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(variational_dist_u, Delta):\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(\n\u001B[0;32m    133\u001B[0m         x, inducing_points, inducing_values\u001B[38;5;241m=\u001B[39mvariational_dist_u\u001B[38;5;241m.\u001B[39mmean, variational_inducing_covar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    134\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\module.py:30\u001B[0m, in \u001B[0;36mModule.__call__\u001B[1;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 30\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m     32\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [_validate_module_outputs(output) \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m outputs]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\variational\\variational_strategy.py:131\u001B[0m, in \u001B[0;36mVariationalStrategy.forward\u001B[1;34m(self, x, inducing_points, inducing_values, variational_inducing_covar, **kwargs)\u001B[0m\n\u001B[0;32m    125\u001B[0m     predictive_covar \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    126\u001B[0m         data_data_covar\u001B[38;5;241m.\u001B[39madd_jitter(\u001B[38;5;241m1e-4\u001B[39m)\u001B[38;5;241m.\u001B[39mevaluate()\n\u001B[0;32m    127\u001B[0m         \u001B[38;5;241m+\u001B[39m interp_term\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m@\u001B[39m middle_term\u001B[38;5;241m.\u001B[39mevaluate() \u001B[38;5;241m@\u001B[39m interp_term\n\u001B[0;32m    128\u001B[0m     )\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    130\u001B[0m     predictive_covar \u001B[38;5;241m=\u001B[39m SumLazyTensor(\n\u001B[1;32m--> 131\u001B[0m         \u001B[43mdata_data_covar\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_jitter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1e-4\u001B[39;49m\u001B[43m)\u001B[49m,\n\u001B[0;32m    132\u001B[0m         MatmulLazyTensor(interp_term\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m), middle_term \u001B[38;5;241m@\u001B[39m interp_term),\n\u001B[0;32m    133\u001B[0m     )\n\u001B[0;32m    135\u001B[0m \u001B[38;5;66;03m# Return the distribution\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m MultivariateNormal(predictive_mean, predictive_covar)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:277\u001B[0m, in \u001B[0;36mLazyEvaluatedKernelTensor.add_jitter\u001B[1;34m(self, jitter_val)\u001B[0m\n\u001B[0;32m    276\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madd_jitter\u001B[39m(\u001B[38;5;28mself\u001B[39m, jitter_val\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m):\n\u001B[1;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate_kernel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39madd_jitter(jitter_val)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\utils\\memoize.py:59\u001B[0m, in \u001B[0;36m_cached.<locals>.g\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     57\u001B[0m kwargs_pkl \u001B[38;5;241m=\u001B[39m pickle\u001B[38;5;241m.\u001B[39mdumps(kwargs)\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_in_cache(\u001B[38;5;28mself\u001B[39m, cache_name, \u001B[38;5;241m*\u001B[39margs, kwargs_pkl\u001B[38;5;241m=\u001B[39mkwargs_pkl):\n\u001B[1;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _add_to_cache(\u001B[38;5;28mself\u001B[39m, cache_name, method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs), \u001B[38;5;241m*\u001B[39margs, kwargs_pkl\u001B[38;5;241m=\u001B[39mkwargs_pkl)\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _get_from_cache(\u001B[38;5;28mself\u001B[39m, cache_name, \u001B[38;5;241m*\u001B[39margs, kwargs_pkl\u001B[38;5;241m=\u001B[39mkwargs_pkl)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:332\u001B[0m, in \u001B[0;36mLazyEvaluatedKernelTensor.evaluate_kernel\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    330\u001B[0m     temp_active_dims \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel\u001B[38;5;241m.\u001B[39mactive_dims\n\u001B[0;32m    331\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel\u001B[38;5;241m.\u001B[39mactive_dims \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 332\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel(\n\u001B[0;32m    333\u001B[0m         x1,\n\u001B[0;32m    334\u001B[0m         x2,\n\u001B[0;32m    335\u001B[0m         diag\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    336\u001B[0m         last_dim_is_batch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_dim_is_batch,\n\u001B[0;32m    337\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams,\n\u001B[0;32m    338\u001B[0m     )\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel\u001B[38;5;241m.\u001B[39mactive_dims \u001B[38;5;241m=\u001B[39m temp_active_dims\n\u001B[0;32m    341\u001B[0m \u001B[38;5;66;03m# Check the size of the output\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\kernels\\kernel.py:402\u001B[0m, in \u001B[0;36mKernel.__call__\u001B[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001B[0m\n\u001B[0;32m    400\u001B[0m     res \u001B[38;5;241m=\u001B[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, last_dim_is_batch\u001B[38;5;241m=\u001B[39mlast_dim_is_batch, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m    401\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 402\u001B[0m     res \u001B[38;5;241m=\u001B[39m lazify(\u001B[38;5;28msuper\u001B[39m(Kernel, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(x1_, x2_, last_dim_is_batch\u001B[38;5;241m=\u001B[39mlast_dim_is_batch, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams))\n\u001B[0;32m    403\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\module.py:30\u001B[0m, in \u001B[0;36mModule.__call__\u001B[1;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 30\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m     32\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [_validate_module_outputs(output) \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m outputs]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\kernels\\kernel.py:469\u001B[0m, in \u001B[0;36mAdditiveKernel.forward\u001B[1;34m(self, x1, x2, diag, **params)\u001B[0m\n\u001B[0;32m    467\u001B[0m res \u001B[38;5;241m=\u001B[39m ZeroLazyTensor() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m diag \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    468\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m kern \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernels:\n\u001B[1;32m--> 469\u001B[0m     next_term \u001B[38;5;241m=\u001B[39m kern(x1, x2, diag\u001B[38;5;241m=\u001B[39mdiag, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m    470\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m diag:\n\u001B[0;32m    471\u001B[0m         res \u001B[38;5;241m=\u001B[39m res \u001B[38;5;241m+\u001B[39m lazify(next_term)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\kernels\\kernel.py:402\u001B[0m, in \u001B[0;36mKernel.__call__\u001B[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001B[0m\n\u001B[0;32m    400\u001B[0m     res \u001B[38;5;241m=\u001B[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, last_dim_is_batch\u001B[38;5;241m=\u001B[39mlast_dim_is_batch, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m    401\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 402\u001B[0m     res \u001B[38;5;241m=\u001B[39m lazify(\u001B[38;5;28msuper\u001B[39m(Kernel, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(x1_, x2_, last_dim_is_batch\u001B[38;5;241m=\u001B[39mlast_dim_is_batch, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams))\n\u001B[0;32m    403\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\module.py:30\u001B[0m, in \u001B[0;36mModule.__call__\u001B[1;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 30\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m     32\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [_validate_module_outputs(output) \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m outputs]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\kernels\\spectral_mixture_kernel.py:335\u001B[0m, in \u001B[0;36mSpectralMixtureKernel.forward\u001B[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001B[0m\n\u001B[0;32m    332\u001B[0m x1_cos_, x2_cos_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_input_grid(x1_cos, x2_cos, diag\u001B[38;5;241m=\u001B[39mdiag, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m    334\u001B[0m \u001B[38;5;66;03m# Compute the exponential and cosine terms\u001B[39;00m\n\u001B[1;32m--> 335\u001B[0m exp_term \u001B[38;5;241m=\u001B[39m \u001B[43m(\u001B[49m\u001B[43mx1_exp_\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mx2_exp_\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpow_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mmul_(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m math\u001B[38;5;241m.\u001B[39mpi \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    336\u001B[0m cos_term \u001B[38;5;241m=\u001B[39m (x1_cos_ \u001B[38;5;241m-\u001B[39m x2_cos_)\u001B[38;5;241m.\u001B[39mmul_(\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m math\u001B[38;5;241m.\u001B[39mpi)\n\u001B[0;32m    337\u001B[0m res \u001B[38;5;241m=\u001B[39m exp_term\u001B[38;5;241m.\u001B[39mexp_() \u001B[38;5;241m*\u001B[39m cos_term\u001B[38;5;241m.\u001B[39mcos_()\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 4.33 GiB (GPU 0; 8.00 GiB total capacity; 4.78 GiB already allocated; 949.99 MiB free; 5.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from utils import get_BIC\n",
    "\n",
    "print(get_BIC(m1, likelihood, train_y, train_x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m1.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred = likelihood(m1(test_x))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize plot\n",
    "f, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "\n",
    "# Get upper and lower confidence bounds\n",
    "lower, upper = observed_pred.confidence_region()\n",
    "# Shade between the lower and upper confidence bounds\n",
    "ax.fill_between(test_x.detach().cpu().numpy(),\n",
    "                lower.detach().cpu().numpy(),\n",
    "                upper.detach().cpu().numpy(), alpha=0.3)\n",
    "# Plot training data as black stars\n",
    "ax.scatter(train_x.detach().cpu().numpy(), train_y.detach().cpu().numpy(), s=0.5)\n",
    "# Plot predictive means as blue line\n",
    "ax.plot(test_x.detach().cpu().numpy(), observed_pred.mean.detach().cpu().numpy(), 'blue')\n",
    "\n",
    "ax.set_ylim([0, 1.5])\n",
    "#ax.patch.set_facecolor('green')\n",
    "#ax.patch.set_alpha(.1)\n",
    "ax.legend([\"95% Credible Intervals\", \"Observed Data\", \"Posterior Mean\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
