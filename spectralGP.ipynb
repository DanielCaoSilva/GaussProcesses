{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm.notebook\n",
    "import time\n",
    "import DataGrabber\n",
    "import utils\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "plt.style.use('classic')\n",
    "\n",
    "from utils import set_gpytorch_settings\n",
    "#gpytorch.settings.max_cholesky_size\n",
    "set_gpytorch_settings()\n",
    "\n",
    "# Command in terminal to help with memory allocation\n",
    "# set PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Kernel Imports\n",
    "from gpytorch.kernels import PeriodicKernel\n",
    "from custom_kernel import MinKernel, AR2Kernel, MaternKernel, LinearKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "from gpytorch.constraints import Interval\n",
    "from custom_kernel import noise_lower, noise_upper, noise_init"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Grab Data and create tensors\n",
    "data_grab = DataGrabber.DataGrab(year=[\"2022\", \"2021\", \"2020\"])\n",
    "wave_data = data_grab.grab_data()\n",
    "y = torch.tensor(wave_data['Wave Height'].values.astype(np.float32)).cuda()\n",
    "X = torch.tensor(wave_data['Time'].values.astype(np.float32)).cuda()\n",
    "X = X.reshape(-1,1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Standardize Data\n",
    "X = (X - X.min(0).values) / (X.max(0).values - X.min(0).values)\n",
    "y = y.log()\n",
    "y = y - torch.min(y)\n",
    "y = 2 * (y / torch.max(y)) - 1\n",
    "# Training vs test\n",
    "from math import floor\n",
    "train_n = int(floor(0.7 * len(X)))\n",
    "train_x = X[:train_n].contiguous().cuda()\n",
    "train_y = y[:train_n].contiguous().cuda()\n",
    "test_x = X[train_n:].contiguous().cuda()\n",
    "test_y = y[train_n:].contiguous().cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Generate the train_loader and train_dataset\n",
    "train_loader, train_dataset, test_loader, test_dataset = utils.create_train_loader_and_dataset(\n",
    "    train_x, train_y, test_x, test_y)\n",
    "data_compact = [train_x, train_y, test_x, test_y, train_loader, train_dataset, test_loader, test_dataset]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class StandardApproximateGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, kernel):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(-2))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "class MeanFieldApproximateGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, kernel):\n",
    "        variational_distribution = gpytorch.variational.MeanFieldVariationalDistribution(inducing_points.size(-2))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "class MAPApproximateGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, kernel):\n",
    "        variational_distribution = gpytorch.variational.DeltaVariationalDistribution(inducing_points.size(-2))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "def make_orthogonal_vs(model, train_x):\n",
    "    mean_inducing_points = torch.randn(1000, train_x.size(-1), dtype=train_x.dtype, device=train_x.device)\n",
    "    covar_inducing_points = torch.randn(100, train_x.size(-1), dtype=train_x.dtype, device=train_x.device)\n",
    "\n",
    "    covar_variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "        model, covar_inducing_points,\n",
    "        gpytorch.variational.CholeskyVariationalDistribution(covar_inducing_points.size(-2)),\n",
    "        learn_inducing_locations=True\n",
    "    )\n",
    "\n",
    "    variational_strategy = gpytorch.variational.OrthogonallyDecoupledVariationalStrategy(\n",
    "        covar_variational_strategy, mean_inducing_points,\n",
    "        gpytorch.variational.DeltaVariationalDistribution(mean_inducing_points.size(-2)),\n",
    "    )\n",
    "    return variational_strategy\n",
    "\n",
    "class OrthDecoupledApproximateGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, kernel):\n",
    "        variational_distribution = gpytorch.variational.DeltaVariationalDistribution(inducing_points.size(-2))\n",
    "        variational_strategy = make_orthogonal_vs(self, train_x)\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class SpectralDeltaGP(gpytorch.models.ExactGP):\n",
    "    # def __init__(self, train_x, train_y, kernel, num_deltas, noise_init=None):\n",
    "    #     likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint=gpytorch.constraints.GreaterThan(1e-11))\n",
    "    #     likelihood.register_prior(\"noise_prior\", gpytorch.priors.HorseshoePrior(0.1), \"noise\")\n",
    "    #     likelihood.noise = 1e-2\n",
    "    def __init__(self, inducing_points, kernel):\n",
    "        variational_distribution = gpytorch.variational.DeltaVariationalDistribution(inducing_points)\n",
    "        #variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(-2))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        #super(SpectralDeltaGP, self).__init__(train_x, train_y, likelihood)\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        #base_covar_module = kernel #gpytorch.kernels.SpectralDeltaKernel(num_dims=train_x.size(-1), num_deltas=num_deltas)\n",
    "        #base_covar_module.initialize_from_data(train_x[0], train_y[0])\n",
    "        self.covar_module = kernel#gpytorch.kernels.ScaleKernel(base_covar_module)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class SpectralMixtureGPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, kernel, number_of_mixtures=4):#,  train_x, train_y, likelihood):\n",
    "        variational_distribution = gpytorch.variational.DeltaVariationalDistribution(inducing_points.size(-2))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super(SpectralMixtureGPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=number_of_mixtures)+kernel\n",
    "        #self.covar_module.initialize_from_data(train_x, train_y)\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "#likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "#model = SpectralMixtureGPModel(train_x, train_y, likelihood)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "num_deltas = 300\n",
    "kernel_old = (\n",
    "        #ScaleKernel(AR2Kernel()) +\n",
    "        #ScaleKernel(MinKernel()*RBFKernel()) +\n",
    "        #ScaleKernel(MinKernel())+\n",
    "        #ScaleKernel(RBFKernel()) +\n",
    "        #ScaleKernel(RBFKernel()*LinearKernel())+\n",
    "        ScaleKernel(MaternKernel(nu=0.5)) +\n",
    "        ScaleKernel(MaternKernel(nu=1.5)*PeriodicKernel(period_length_constraint=Interval(\n",
    "            lower_bound=1e-4,\n",
    "            upper_bound=0.1,\n",
    "            initial_value=0.01\n",
    "        ))) +\n",
    "        ScaleKernel(MaternKernel(nu=1.5)*PeriodicKernel(period_length_constraint=Interval(\n",
    "            lower_bound=0.05,\n",
    "            upper_bound=0.3,\n",
    "            initial_value=0.15\n",
    "        ))) +\n",
    "        ScaleKernel(MaternKernel(nu=1.5)*PeriodicKernel(period_length_constraint=Interval(\n",
    "            lower_bound=0.25,\n",
    "            upper_bound=1,\n",
    "            initial_value=0.5\n",
    "        ))) +\n",
    "        ScaleKernel(gpytorch.kernels.SpectralDeltaKernel(\n",
    "            num_dims=train_x.size(-1),\n",
    "            num_deltas=num_deltas,\n",
    "        )) +\n",
    "        ScaleKernel(gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4))\n",
    "        #ScaleKernel(RBFKernel()*PeriodicKernel())\n",
    "        )\n",
    "kernel = (\n",
    "    ScaleKernel(LinearKernel()*gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4))\n",
    ")\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = Interval(noise_lower, noise_upper,initial_value=noise_init))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Training SpectralMixtureGPModel:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ef71c1df1f1413783d048b96209d41f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dcaos\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\lazy\\triangular_lazy_tensor.py:130: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:1672.)\n",
      "  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test SpectralMixtureGPModel MAE: 0.12376188486814499\n",
      "--- 93.72667503356934 seconds ---\n"
     ]
    }
   ],
   "source": [
    "num_ind_pts = 150 # Number of inducing points (128 is default for train_and_test_approximate_gp function)\n",
    "\n",
    "#m1, l1 = utils.train_and_test_approximate_gp(\n",
    "#    StandardApproximateGP, kernel, *data_compact, num_epochs=100, num_ind_pts=num_ind_pts)\n",
    "# m2, l2 = utils.train_and_test_approximate_gp(\n",
    "#     MeanFieldApproximateGP, kernel, *data_compact, num_epochs=100, num_ind_pts=num_ind_pts)\n",
    "# m3, l3 = utils.train_and_test_approximate_gp(\n",
    "#     MAPApproximateGP, kernel, *data_compact, num_epochs=100, num_ind_pts=num_ind_pts)\n",
    "# m4, l4 = utils.train_and_test_approximate_gp(\n",
    "#     OrthDecoupledApproximateGP, kernel, *data_compact, num_epochs=100, num_ind_pts=num_ind_pts)\n",
    "# m5, l5 = utils.train_and_test_approximate_gp(\n",
    "#     SpectralDeltaGP, kernel, *data_compact, num_epochs=100, num_ind_pts=num_deltas)\n",
    "# m6, l6 = utils.train_and_test_approximate_gp(\n",
    "#     SpectralMixtureGPModel, kernel, *data_compact, num_epochs=100, num_ind_pts=num_deltas)\n",
    "#l1 = gpytorch.likelihoods.GaussianLikelihood()\n",
    "#m1 = SpectralMixtureGPModel(train_x, train_y, likelihood)\n",
    "m1, l1 = utils.train_and_test_approximate_gp(\n",
    "    SpectralMixtureGPModel, kernel, *data_compact, num_epochs=100, num_ind_pts=num_ind_pts)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.92 GiB (GPU 0; 8.00 GiB total capacity; 5.86 GiB already allocated; 507.00 MiB free; 5.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Make predictions by feeding model through likelihood\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad(), gpytorch\u001B[38;5;241m.\u001B[39msettings\u001B[38;5;241m.\u001B[39mfast_pred_var():\n\u001B[1;32m---> 12\u001B[0m     observed_pred \u001B[38;5;241m=\u001B[39m likelihood(\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_x\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Initialize plot\u001B[39;00m\n\u001B[0;32m     14\u001B[0m f, ax \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39msubplots(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m7\u001B[39m))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\models\\approximate_gp.py:81\u001B[0m, in \u001B[0;36mApproximateGP.__call__\u001B[1;34m(self, inputs, prior, **kwargs)\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m     80\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 81\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvariational_strategy(inputs, prior\u001B[38;5;241m=\u001B[39mprior, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\variational\\variational_strategy.py:169\u001B[0m, in \u001B[0;36mVariationalStrategy.__call__\u001B[1;34m(self, x, prior, **kwargs)\u001B[0m\n\u001B[0;32m    166\u001B[0m         \u001B[38;5;66;03m# Mark that we have updated the variational strategy\u001B[39;00m\n\u001B[0;32m    167\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdated_strategy\u001B[38;5;241m.\u001B[39mfill_(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m--> 169\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(x, prior\u001B[38;5;241m=\u001B[39mprior, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\variational\\_variational_strategy.py:132\u001B[0m, in \u001B[0;36m_VariationalStrategy.__call__\u001B[1;34m(self, x, prior, **kwargs)\u001B[0m\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(\n\u001B[0;32m    125\u001B[0m         x,\n\u001B[0;32m    126\u001B[0m         inducing_points,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    129\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    130\u001B[0m     )\n\u001B[0;32m    131\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(variational_dist_u, Delta):\n\u001B[1;32m--> 132\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(\n\u001B[0;32m    133\u001B[0m         x, inducing_points, inducing_values\u001B[38;5;241m=\u001B[39mvariational_dist_u\u001B[38;5;241m.\u001B[39mmean, variational_inducing_covar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    134\u001B[0m     )\n\u001B[0;32m    135\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    136\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    137\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid variational distribuition (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(variational_dist_u)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m). \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    138\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected a multivariate normal or a delta distribution.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    139\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\module.py:30\u001B[0m, in \u001B[0;36mModule.__call__\u001B[1;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 30\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m     32\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [_validate_module_outputs(output) \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m outputs]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\variational\\variational_strategy.py:131\u001B[0m, in \u001B[0;36mVariationalStrategy.forward\u001B[1;34m(self, x, inducing_points, inducing_values, variational_inducing_covar, **kwargs)\u001B[0m\n\u001B[0;32m    125\u001B[0m     predictive_covar \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    126\u001B[0m         data_data_covar\u001B[38;5;241m.\u001B[39madd_jitter(\u001B[38;5;241m1e-4\u001B[39m)\u001B[38;5;241m.\u001B[39mevaluate()\n\u001B[0;32m    127\u001B[0m         \u001B[38;5;241m+\u001B[39m interp_term\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m@\u001B[39m middle_term\u001B[38;5;241m.\u001B[39mevaluate() \u001B[38;5;241m@\u001B[39m interp_term\n\u001B[0;32m    128\u001B[0m     )\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    130\u001B[0m     predictive_covar \u001B[38;5;241m=\u001B[39m SumLazyTensor(\n\u001B[1;32m--> 131\u001B[0m         \u001B[43mdata_data_covar\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_jitter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1e-4\u001B[39;49m\u001B[43m)\u001B[49m,\n\u001B[0;32m    132\u001B[0m         MatmulLazyTensor(interp_term\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m), middle_term \u001B[38;5;241m@\u001B[39m interp_term),\n\u001B[0;32m    133\u001B[0m     )\n\u001B[0;32m    135\u001B[0m \u001B[38;5;66;03m# Return the distribution\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m MultivariateNormal(predictive_mean, predictive_covar)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:277\u001B[0m, in \u001B[0;36mLazyEvaluatedKernelTensor.add_jitter\u001B[1;34m(self, jitter_val)\u001B[0m\n\u001B[0;32m    276\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madd_jitter\u001B[39m(\u001B[38;5;28mself\u001B[39m, jitter_val\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m):\n\u001B[1;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate_kernel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39madd_jitter(jitter_val)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\utils\\memoize.py:59\u001B[0m, in \u001B[0;36m_cached.<locals>.g\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     57\u001B[0m kwargs_pkl \u001B[38;5;241m=\u001B[39m pickle\u001B[38;5;241m.\u001B[39mdumps(kwargs)\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_in_cache(\u001B[38;5;28mself\u001B[39m, cache_name, \u001B[38;5;241m*\u001B[39margs, kwargs_pkl\u001B[38;5;241m=\u001B[39mkwargs_pkl):\n\u001B[1;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _add_to_cache(\u001B[38;5;28mself\u001B[39m, cache_name, method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs), \u001B[38;5;241m*\u001B[39margs, kwargs_pkl\u001B[38;5;241m=\u001B[39mkwargs_pkl)\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _get_from_cache(\u001B[38;5;28mself\u001B[39m, cache_name, \u001B[38;5;241m*\u001B[39margs, kwargs_pkl\u001B[38;5;241m=\u001B[39mkwargs_pkl)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:332\u001B[0m, in \u001B[0;36mLazyEvaluatedKernelTensor.evaluate_kernel\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    330\u001B[0m     temp_active_dims \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel\u001B[38;5;241m.\u001B[39mactive_dims\n\u001B[0;32m    331\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel\u001B[38;5;241m.\u001B[39mactive_dims \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 332\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel(\n\u001B[0;32m    333\u001B[0m         x1,\n\u001B[0;32m    334\u001B[0m         x2,\n\u001B[0;32m    335\u001B[0m         diag\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    336\u001B[0m         last_dim_is_batch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_dim_is_batch,\n\u001B[0;32m    337\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams,\n\u001B[0;32m    338\u001B[0m     )\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel\u001B[38;5;241m.\u001B[39mactive_dims \u001B[38;5;241m=\u001B[39m temp_active_dims\n\u001B[0;32m    341\u001B[0m \u001B[38;5;66;03m# Check the size of the output\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\kernels\\kernel.py:402\u001B[0m, in \u001B[0;36mKernel.__call__\u001B[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001B[0m\n\u001B[0;32m    400\u001B[0m     res \u001B[38;5;241m=\u001B[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, last_dim_is_batch\u001B[38;5;241m=\u001B[39mlast_dim_is_batch, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m    401\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 402\u001B[0m     res \u001B[38;5;241m=\u001B[39m lazify(\u001B[38;5;28msuper\u001B[39m(Kernel, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(x1_, x2_, last_dim_is_batch\u001B[38;5;241m=\u001B[39mlast_dim_is_batch, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams))\n\u001B[0;32m    403\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\module.py:30\u001B[0m, in \u001B[0;36mModule.__call__\u001B[1;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 30\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m     32\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [_validate_module_outputs(output) \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m outputs]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\kernels\\kernel.py:469\u001B[0m, in \u001B[0;36mAdditiveKernel.forward\u001B[1;34m(self, x1, x2, diag, **params)\u001B[0m\n\u001B[0;32m    467\u001B[0m res \u001B[38;5;241m=\u001B[39m ZeroLazyTensor() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m diag \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    468\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m kern \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernels:\n\u001B[1;32m--> 469\u001B[0m     next_term \u001B[38;5;241m=\u001B[39m kern(x1, x2, diag\u001B[38;5;241m=\u001B[39mdiag, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m    470\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m diag:\n\u001B[0;32m    471\u001B[0m         res \u001B[38;5;241m=\u001B[39m res \u001B[38;5;241m+\u001B[39m lazify(next_term)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\kernels\\kernel.py:402\u001B[0m, in \u001B[0;36mKernel.__call__\u001B[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001B[0m\n\u001B[0;32m    400\u001B[0m     res \u001B[38;5;241m=\u001B[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, last_dim_is_batch\u001B[38;5;241m=\u001B[39mlast_dim_is_batch, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m    401\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 402\u001B[0m     res \u001B[38;5;241m=\u001B[39m lazify(\u001B[38;5;28msuper\u001B[39m(Kernel, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(x1_, x2_, last_dim_is_batch\u001B[38;5;241m=\u001B[39mlast_dim_is_batch, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams))\n\u001B[0;32m    403\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\module.py:30\u001B[0m, in \u001B[0;36mModule.__call__\u001B[1;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 30\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m     32\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [_validate_module_outputs(output) \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m outputs]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPs\\lib\\site-packages\\gpytorch\\kernels\\spectral_mixture_kernel.py:337\u001B[0m, in \u001B[0;36mSpectralMixtureKernel.forward\u001B[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001B[0m\n\u001B[0;32m    335\u001B[0m exp_term \u001B[38;5;241m=\u001B[39m (x1_exp_ \u001B[38;5;241m-\u001B[39m x2_exp_)\u001B[38;5;241m.\u001B[39mpow_(\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mmul_(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m math\u001B[38;5;241m.\u001B[39mpi \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    336\u001B[0m cos_term \u001B[38;5;241m=\u001B[39m (x1_cos_ \u001B[38;5;241m-\u001B[39m x2_cos_)\u001B[38;5;241m.\u001B[39mmul_(\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m math\u001B[38;5;241m.\u001B[39mpi)\n\u001B[1;32m--> 337\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mexp_term\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexp_\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcos_term\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcos_\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    339\u001B[0m \u001B[38;5;66;03m# Sum over mixtures\u001B[39;00m\n\u001B[0;32m    340\u001B[0m mixture_weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmixture_weights\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmixture_weights\u001B[38;5;241m.\u001B[39mshape, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 2.92 GiB (GPU 0; 8.00 GiB total capacity; 5.86 GiB already allocated; 507.00 MiB free; 5.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "pairs = [[m1, l1],]\n",
    "#         [m2, l2], [m3, l3],\n",
    "#         [m4, l4], [m5, l5],\n",
    "        #[m1, l1]]\n",
    "\n",
    "for pair in pairs:\n",
    "    model = pair[0]\n",
    "    likelihood = pair[1]\n",
    "    model.eval()\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x[:,0].detach().cpu().numpy(),\n",
    "                    lower.detach().cpu().numpy(),\n",
    "                    upper.detach().cpu().numpy(), alpha=0.3)\n",
    "    # Plot training data as black stars\n",
    "    ax.scatter(train_x[:,0].detach().cpu().numpy(), train_y.detach().cpu().numpy(), s=0.5)\n",
    "    #ax.scatter(model.variational_strategy.inducing_points[:,0].detach().cpu().numpy(),\n",
    "    #           np.zeros(500)+1, s=0.5)\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x[:,0].detach().cpu().numpy(), observed_pred.mean.detach().cpu().numpy(), 'blue')\n",
    "    ax.scatter(\n",
    "        test_x[:,0].detach().cpu().numpy(),\n",
    "        test_y.detach().cpu().numpy(),\n",
    "        s=1, color=\"red\")\n",
    "    ax.set_xlim(0,1)\n",
    "    #ax.set_xlim(0.65,0.75)\n",
    "    ax.vlines(m1.variational_strategy.inducing_points.detach().cpu().numpy(), ymin = -1.6, ymax = -1.5)\n",
    "\n",
    "    #ax.set_ylim([0, 1.5])\n",
    "    #ax.patch.set_facecolor('green')\n",
    "    #ax.patch.set_alpha(.1)\n",
    "    ax.legend([\"95% Credible Intervals\", \"Observed Data\", \"Posterior Mean\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |    5997 MB |    5997 MB |    3070 GB |    3064 GB |\\n|       from large pool |    5996 MB |    5996 MB |    2957 GB |    2952 GB |\\n|       from small pool |       1 MB |      11 MB |     112 GB |     112 GB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |    5997 MB |    5997 MB |    3070 GB |    3064 GB |\\n|       from large pool |    5996 MB |    5996 MB |    2957 GB |    2952 GB |\\n|       from small pool |       1 MB |      11 MB |     112 GB |     112 GB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |    6018 MB |    6418 MB |    6418 MB |  409600 KB |\\n|       from large pool |    6012 MB |    6404 MB |    6404 MB |  401408 KB |\\n|       from small pool |       6 MB |      14 MB |      14 MB |    8192 KB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |   20743 KB |   66717 KB |    1023 GB |    1023 GB |\\n|       from large pool |   16366 KB |   60448 KB |     901 GB |     901 GB |\\n|       from small pool |    4377 KB |   12504 KB |     121 GB |     121 GB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     123    |     249    |    2307 K  |    2307 K  |\\n|       from large pool |       4    |      32    |     364 K  |     364 K  |\\n|       from small pool |     119    |     221    |    1942 K  |    1942 K  |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     123    |     249    |    2307 K  |    2307 K  |\\n|       from large pool |       4    |      32    |     364 K  |     364 K  |\\n|       from small pool |     119    |     221    |    1942 K  |    1942 K  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       7    |      30    |      30    |      23    |\\n|       from large pool |       4    |      23    |      23    |      19    |\\n|       from small pool |       3    |       7    |       7    |       4    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      17    |      36    |    1221 K  |    1221 K  |\\n|       from large pool |       2    |      12    |     154 K  |     154 K  |\\n|       from small pool |      15    |      30    |    1067 K  |    1067 K  |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(len(m1.covar_module.kernels)):\n",
    "#     print(m1.covar_module.kernels[i].outputscale)\n",
    "torch.cuda.memory_summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import get_BIC\n",
    "\n",
    "print(get_BIC(m1, likelihood, train_y, train_x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m1.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred = likelihood(m1(test_x))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize plot\n",
    "f, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "\n",
    "# Get upper and lower confidence bounds\n",
    "lower, upper = observed_pred.confidence_region()\n",
    "# Shade between the lower and upper confidence bounds\n",
    "ax.fill_between(test_x.detach().cpu().numpy(),\n",
    "                lower.detach().cpu().numpy(),\n",
    "                upper.detach().cpu().numpy(), alpha=0.3)\n",
    "# Plot training data as black stars\n",
    "ax.scatter(train_x.detach().cpu().numpy(), train_y.detach().cpu().numpy(), s=0.5)\n",
    "# Plot predictive means as blue line\n",
    "ax.plot(test_x.detach().cpu().numpy(), observed_pred.mean.detach().cpu().numpy(), 'blue')\n",
    "\n",
    "ax.set_ylim([0, 1.5])\n",
    "#ax.patch.set_facecolor('green')\n",
    "#ax.patch.set_alpha(.1)\n",
    "ax.legend([\"95% Credible Intervals\", \"Observed Data\", \"Posterior Mean\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}